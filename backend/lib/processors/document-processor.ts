/**
 * æ–‡æ¡£å¤„ç†æµç¨‹
 *
 * å®Œæ•´çš„æ–‡æ¡£ä¸Šä¼ åå¤„ç†æµç¨‹ï¼š
 * 1. ä» COS ä¸‹è½½æ–‡ä»¶
 * 2. è§£ææ–‡ä»¶å†…å®¹
 * 3. K-Type è®¤çŸ¥åˆ†æ
 * 4. çˆ¶å­åˆ†å—
 * 5. Embedding ç”Ÿæˆ
 * 6. ä¸‰å±‚å†™å…¥ Qdrant
 *
 * @module lib/processors/document-processor
 */

import COS from 'cos-nodejs-sdk-v5'
import { v4 as uuidv4 } from 'uuid'
import {
  processKTypeWorkflowEfficient,
  type KTypeProcessResult,
  KTypeSafetyError,
} from './k-type-efficient-vercel'
import {
  splitIntoParentChildChunksBatch,
  splitIntoParentChildChunksStream,
} from '../chunkers/parent-child'
import embeddingClient from '../embedding'
import { ensureUserCollection, upsertPoints, type VectorPoint } from '../qdrant'
import {
  updateDocumentStatus,
  updateDocumentKType,
  type Document,
} from '../db/queries'
import { parsePDF } from '../parsers/pdf'
import { parseDOCX } from '../parsers/docx'
import { parseTXT } from '../parsers/text'
import { base64ToBuffer } from '../storage/local'
import { runSemchunk } from '../semchunk'
import { downloadFileFromCOS } from '../storage/cos'
import { incrementCounter, recordTiming } from '../observability/metrics'
import { ENV, parseIntEnv, parseBoolEnv } from '../config/env-helpers'

// ==================== é…ç½® ====================

const cos = new COS({
  SecretId: process.env.TENCENT_COS_SECRET_ID || '',
  SecretKey: process.env.TENCENT_COS_SECRET_KEY || '',
})

const BUCKET = process.env.TENCENT_COS_BUCKET || ''
const REGION = process.env.TENCENT_COS_REGION || 'ap-guangzhou'

// ä½¿ç”¨ç»Ÿä¸€çš„ç¯å¢ƒå˜é‡è§£æå·¥å…·
const KTYPE_MAX_TOKENS = ENV.KTYPE_MAX_TOKENS
const DOC_CHUNK_SIZE = ENV.DOC_CHUNK_SIZE
const DOC_CHUNK_OVERLAP = ENV.DOC_CHUNK_OVERLAP
const PARENT_CHUNK_SIZE = ENV.PARENT_CHUNK_SIZE
const PARENT_CHUNK_OVERLAP = parseIntEnv('PARENT_CHUNK_OVERLAP', 240)
const CHILD_CHUNK_SIZE = parseIntEnv('CHILD_CHUNK_SIZE', 420)
const CHILD_CHUNK_OVERLAP = parseIntEnv('CHILD_CHUNK_OVERLAP', 100)
const MEMORY_THRESHOLD_MB = parseIntEnv('MEMORY_THRESHOLD_MB', 0)
const MEMORY_LOG = parseBoolEnv('MEMORY_LOG', false)
const GC_AFTER_KTYPE = parseBoolEnv('GC_AFTER_KTYPE', false)
const GC_AFTER_CHUNKING = parseBoolEnv('GC_AFTER_CHUNKING', false)
const GC_AFTER_EMBEDDING = parseBoolEnv('GC_AFTER_EMBEDDING', false)
const CHUNK_STREAMING = parseBoolEnv('CHUNK_STREAMING', false)

function logMemoryUsage(stage: string): void {
  if (!MEMORY_LOG) return
  const usage = process.memoryUsage()
  const rssMB = Math.round(usage.rss / 1024 / 1024)
  const heapMB = Math.round(usage.heapUsed / 1024 / 1024)
  console.log(`ğŸ’¾ [MEM] ${stage}: RSS=${rssMB}MB Heap=${heapMB}MB`)
}

function maybeForceGc(stage: string, force = false): void {
  if (typeof global.gc !== 'function') {
    return
  }

  const usage = process.memoryUsage()
  const rssMB = Math.round(usage.rss / 1024 / 1024)
  const shouldGc = force || (MEMORY_THRESHOLD_MB > 0 && rssMB >= MEMORY_THRESHOLD_MB)

  if (!shouldGc) {
    return
  }

  console.log(`ğŸ§¹ [GC] ${stage}: rss=${rssMB}MB`)
  global.gc()
  logMemoryUsage(`${stage}-after-gc`)
}

function splitTextByLength(text: string, chunkSize: number, overlap: number): string[] {
  if (!text) return []
  if (chunkSize <= 0 || text.length <= chunkSize) return [text]
  const safeOverlap = Math.max(0, overlap)
  const step = Math.max(1, chunkSize - safeOverlap)
  const chunks: string[] = []

  for (let start = 0; start < text.length; start += step) {
    const end = Math.min(text.length, start + chunkSize)
    const chunk = text.slice(start, end)
    if (chunk.trim()) {
      chunks.push(chunk)
    }
    if (end >= text.length) break
  }

  return chunks.length > 0 ? chunks : [text]
}

function buildKTypeDocText(report: KTypeProcessResult['finalReport']): string {
  const fullReport = (report.distilledContent || '').trim()
  if (fullReport) return fullReport
  return (report.executiveSummary || '').trim()
}

// ==================== ç±»å‹å®šä¹‰ ====================

export interface ProcessingOptions {
  // K-Type åˆ†æé€‰é¡¹
  skipKType?: boolean

  // åˆ†å—é€‰é¡¹
  docChunkSize?: number
  docChunkOverlap?: number
  parentChunkSize?: number
  parentChunkOverlap?: number
  childChunkSize?: number
  childChunkOverlap?: number

  // Embedding é€‰é¡¹
  embeddingBatchSize?: number
}

export interface ProcessingResult {
  success: boolean
  documentId: string
  processed: boolean
  error?: string
  stats?: {
    textLength: number
    parentChunks: number
    childChunks: number
    embeddingTime: number
  }
}

export interface ProcessingProgress {
  documentId: string
  status: 'downloading' | 'parsing' | 'ktype' | 'chunking' | 'embedding' | 'qdrant' | 'completed' | 'failed'
  progress: number // 0-100
  message: string
  error?: string
}

// ==================== ä¸»å¤„ç†æµç¨‹ ====================

/**
 * æ ¸å¿ƒæ–‡æ¡£å¤„ç†æµç¨‹ï¼ˆç»Ÿä¸€çš„å†…éƒ¨å®ç°ï¼‰
 *
 * è¿™ä¸ªå‡½æ•°åŒ…å«äº†æ‰€æœ‰æ–‡æ¡£å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
 * ä¸¤ä¸ªå…¬å…±æ¥å£ processDocument å’Œ processDocumentWithText éƒ½è°ƒç”¨è¿™ä¸ªå‡½æ•°
 *
 * @param document - æ–‡æ¡£ä¿¡æ¯
 * @param textContent - æ–‡æœ¬å†…å®¹ï¼ˆå·²æå–ï¼‰
 * @param options - å¤„ç†é€‰é¡¹
 * @param onProgress - è¿›åº¦å›è°ƒ
 * @param startProgress - èµ·å§‹è¿›åº¦å€¼ï¼ˆç”¨äºä¸åŒå…¥å£çš„è¿›åº¦è°ƒæ•´ï¼‰
 * @returns å¤„ç†ç»“æœ
 */
async function processDocumentCore(
  document: Document,
  textContent: string,
  options: ProcessingOptions,
  onProgress?: (progress: ProcessingProgress) => void,
  startProgress = 0
): Promise<ProcessingResult> {
  const {
    skipKType = false,
    docChunkSize = DOC_CHUNK_SIZE,
    docChunkOverlap = DOC_CHUNK_OVERLAP,
    parentChunkSize = PARENT_CHUNK_SIZE,
    parentChunkOverlap = PARENT_CHUNK_OVERLAP,
    childChunkSize = CHILD_CHUNK_SIZE,
    childChunkOverlap = CHILD_CHUNK_OVERLAP,
    embeddingBatchSize = 10,
  } = options

  try {
    console.log(`ğŸ“„ [Processor] å¼€å§‹å¤„ç†æ–‡æ¡£: ${document.file_name} (docId=${document.id})`)
    console.log(`ğŸ“¥ [Processor] æ–‡æœ¬å†…å®¹é•¿åº¦: ${textContent.length} å­—ç¬¦`)

    // 1. K-Type åˆ†æ
    const ktypeResults: KTypeProcessResult[] = []
    if (!skipKType) {
      onProgress?.({
        documentId: document.id,
        status: 'ktype',
        progress: startProgress + 20,
        message: 'K-Type åˆ†æä¸­...',
      })

      try {
        const ktypeInputs = (await runSemchunk({ text: textContent }, KTYPE_MAX_TOKENS)) as string[]
        for (const part of ktypeInputs) {
          const result = await processKTypeWorkflowEfficient(part)
          ktypeResults.push(result)
        }
        if (ktypeResults.length > 0) {
          console.log(`âœ… [Processor] K-Type åˆ†æå®Œæˆ (${ktypeResults.length} parts)`)
          console.log(
            `   ä¸»å¯¼ç±»å‹: ${ktypeResults[0].finalReport.classification.dominantType.join(', ')}`
          )
          console.log(`   çŸ¥è¯†æ¨¡å—: ${ktypeResults[0].finalReport.knowledgeModules.length} ä¸ª`)
        }
      } catch (error) {
        if (error instanceof KTypeSafetyError) {
          console.error(`âŒ [Processor] K-Type è¢«å†…å®¹å®‰å…¨å®¡æ ¸æ‹¦æˆª:`, error.message)
          throw error
        }
        console.warn(`âš ï¸  [Processor] K-Type åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥:`, error)
        try {
          const result = await processKTypeWorkflowEfficient(textContent)
          ktypeResults.push(result)
        } catch (fallbackError) {
          if (fallbackError instanceof KTypeSafetyError) {
            throw fallbackError
          }
          console.error(`âŒ [Processor] K-Type å›é€€å¤±è´¥:`, fallbackError)
          throw new Error(`K-Type åˆ†æå¤±è´¥: ${fallbackError instanceof Error ? fallbackError.message : String(fallbackError)}`)
        }
      }
    }

    logMemoryUsage('after-ktype')
    maybeForceGc('after-ktype', GC_AFTER_KTYPE)

    // 2. åˆ†å— (semchunk)
    const useChunkStreaming = CHUNK_STREAMING
    onProgress?.({
      documentId: document.id,
      status: 'chunking',
      progress: startProgress + 40,
      message: useChunkStreaming ? 'åˆ†å—å¤„ç†ä¸­(æµå¼)...' : 'åˆ†å—å¤„ç†ä¸­...',
    })

    let parentChunks: Array<{ index: number; content: string }> = []
    let childChunks: Array<{ index: number; parentIndex: number; content: string }> = []
    let parentChunkCount = 0
    let childChunkCount = 0
    let chunkStream: AsyncIterable<{
      parentChunks: Array<{ index: number; content: string }>
      childChunks: Array<{ index: number; parentIndex: number; content: string }>
    }> | null = null

    if (!useChunkStreaming) {
      try {
        const parentTexts = (await runSemchunk(
          { text: textContent },
          parentChunkSize,
          parentChunkOverlap
        )) as string[]

        const childLists = (await runSemchunk(
          { texts: parentTexts },
          childChunkSize,
          childChunkOverlap
        )) as string[][]

        parentChunks = parentTexts.map((content, index) => ({ index, content }))
        let childIndex = 0
        for (let i = 0; i < childLists.length; i++) {
          for (const child of childLists[i] || []) {
            childChunks.push({ index: childIndex++, parentIndex: i, content: child })
          }
        }
      } catch (error) {
        console.warn('âš ï¸  [Processor] semchunk åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨ fallback:', error)
        const fallback = await splitIntoParentChildChunksBatch(textContent, {
          parentChunkSize,
          parentChunkOverlap,
          childChunkSize,
          childChunkOverlap,
        })
        parentChunks = fallback.parentChunks
        childChunks = fallback.childChunks
      }

      parentChunkCount = parentChunks.length
      childChunkCount = childChunks.length
      console.log(
        `âœ… [Processor] åˆ†å—å®Œæˆ: ${parentChunks.length} çˆ¶å—, ${childChunks.length} å­å—`
      )
    } else {
      console.log('ğŸ“¦ [Processor] åˆ†å—å°†ä½¿ç”¨æµå¼æ¨¡å¼')
      chunkStream = splitIntoParentChildChunksStream(textContent, {
        parentChunkSize,
        parentChunkOverlap,
        childChunkSize,
        childChunkOverlap,
      })
    }

    logMemoryUsage('after-chunking')
    maybeForceGc('after-chunking', GC_AFTER_CHUNKING)

    // 3. Embedding + Qdrant å†™å…¥
    onProgress?.({
      documentId: document.id,
      status: 'embedding',
      progress: startProgress + 50,
      message: 'ç”Ÿæˆå‘é‡å¹¶å†™å…¥æ•°æ®åº“...',
    })

    const embeddingStartTime = Date.now()

    const combinedKTypeText = ktypeResults.length
      ? ktypeResults
          .map((r) => buildKTypeDocText(r.finalReport))
          .filter(Boolean)
          .join('\n\n')
      : ''

    const deepSummary = combinedKTypeText

    const docChunks: Array<{ content: string; report?: KTypeProcessResult['finalReport'] }> = []

    if (combinedKTypeText.trim()) {
      docChunks.push({ content: combinedKTypeText, report: ktypeResults[0]?.finalReport })
    } else if (textContent.trim()) {
      docChunks.push({ content: textContent })
    }

    if (docChunks.length === 0 && textContent) {
      docChunks.push({ content: textContent })
    }


    const totalEmbedTexts = useChunkStreaming
      ? 0
      : docChunks.length + parentChunks.length + childChunks.length
    const embeddingModel = process.env.EMBEDDING_MODEL || 'qwen3-embedding-4b'
    let processedCount = 0

    await ensureUserCollection(document.user_id)

    const embedAndUpsert = async <T>(
      items: T[],
      getText: (item: T) => string,
      buildPoint: (item: T, index: number, vector: number[]) => VectorPoint
    ) => {
      for (let i = 0; i < items.length; i += embeddingBatchSize) {
        const batchItems = items.slice(i, i + embeddingBatchSize)
        const batchTexts = batchItems.map(getText)

        const response = await (embeddingClient as any).embeddings.create({
          model: embeddingModel,
          input: batchTexts,
        })

        const points = response.data.map((d: any, idx: number) =>
          buildPoint(batchItems[idx], i + idx, d.embedding)
        )

        await upsertPoints(document.user_id, points)

        processedCount += batchItems.length
        const progressIncrement =
          totalEmbedTexts > 0 ? Math.floor((processedCount / totalEmbedTexts) * 40) : 0
        onProgress?.({
          documentId: document.id,
          status: 'embedding',
          progress: startProgress + 50 + progressIncrement,
          message:
            totalEmbedTexts > 0
              ? `å‘é‡åŒ–è¿›åº¦: ${processedCount}/${totalEmbedTexts}`
              : `å‘é‡åŒ–è¿›åº¦: ${processedCount}`,
        })
      }
    }

    await embedAndUpsert(
      docChunks,
      (docChunk) => docChunk.content,
      (docChunk, index, vector) => ({
        id: uuidv4(),
        vector,
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'document',
          content: docChunk.content,
          chunk_index: index,
          metadata: {
            file_name: document.file_name,
          },
        },
      })
    )

    if (!useChunkStreaming) {
      await embedAndUpsert(
        parentChunks,
        (parentChunk) => parentChunk.content,
        (parentChunk, _index, vector) => ({
          id: uuidv4(),
          vector,
          payload: {
            doc_id: document.id,
            kb_id: document.kb_id,
            user_id: document.user_id,
            type: 'parent',
            content: parentChunk.content,
            chunk_index: parentChunk.index,
            metadata: {
              file_name: document.file_name,
            },
          },
        })
      )

      await embedAndUpsert(
        childChunks,
        (childChunk) => childChunk.content,
        (childChunk, _index, vector) => ({
          id: uuidv4(),
          vector,
          payload: {
            doc_id: document.id,
            kb_id: document.kb_id,
            user_id: document.user_id,
            type: 'child',
            parent_id: `parent_${document.id}_${childChunk.parentIndex}`,
            content: childChunk.content,
            chunk_index: childChunk.index,
            metadata: {
              file_name: document.file_name,
              parent_index: childChunk.parentIndex,
            },
          },
        })
      )
    } else if (chunkStream) {
      for await (const batch of chunkStream) {
        if (batch.parentChunks.length > 0) {
          parentChunkCount += batch.parentChunks.length
          await embedAndUpsert(
            batch.parentChunks,
            (parentChunk) => parentChunk.content,
            (parentChunk, _index, vector) => ({
              id: uuidv4(),
              vector,
              payload: {
                doc_id: document.id,
                kb_id: document.kb_id,
                user_id: document.user_id,
                type: 'parent',
                content: parentChunk.content,
                chunk_index: parentChunk.index,
                metadata: {
                  file_name: document.file_name,
                },
              },
            })
          )
        }

        if (batch.childChunks.length > 0) {
          childChunkCount += batch.childChunks.length
          await embedAndUpsert(
            batch.childChunks,
            (childChunk) => childChunk.content,
            (childChunk, _index, vector) => ({
              id: uuidv4(),
              vector,
              payload: {
                doc_id: document.id,
                kb_id: document.kb_id,
                user_id: document.user_id,
                type: 'child',
                parent_id: `parent_${document.id}_${childChunk.parentIndex}`,
                content: childChunk.content,
                chunk_index: childChunk.index,
                metadata: {
                  file_name: document.file_name,
                  parent_index: childChunk.parentIndex,
                },
              },
            })
          )
        }
      }

      console.log(`âœ… [Processor] åˆ†å—å®Œæˆ: ${parentChunkCount} çˆ¶å—, ${childChunkCount} å­å—`)
    }

    const totalPoints = docChunks.length + parentChunkCount + childChunkCount
    const embeddingTime = Date.now() - embeddingStartTime
    recordTiming('embedding', embeddingTime)
    console.log(`âœ… [Processor] å‘é‡åŒ–å®Œæˆ: è€—æ—¶ ${(embeddingTime / 1000).toFixed(2)}s, ${totalPoints} ä¸ªå‘é‡ç‚¹`)

    logMemoryUsage('after-embedding')
    maybeForceGc('after-embedding', GC_AFTER_EMBEDDING)

    // 4. æ›´æ–°æ•°æ®åº“
    await updateDocumentKType(
      document.id,
      combinedKTypeText,
      JSON.stringify(ktypeResults.map((r) => r.finalReport)),
      deepSummary,
      childChunkCount
    )

    onProgress?.({
      documentId: document.id,
      status: 'completed',
      progress: 100,
      message: 'å¤„ç†å®Œæˆ',
    })

    console.log(`âœ¨ [Processor] æ–‡æ¡£å¤„ç†å®Œæˆ: ${document.file_name}`)

    return {
      success: true,
      documentId: document.id,
      processed: true,
      stats: {
        textLength: textContent.length,
        parentChunks: parentChunkCount,
        childChunks: childChunkCount,
        embeddingTime,
      },
    }
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    incrementCounter('document_process_error')
    console.error(`âŒ [Processor] å¤„ç†å¤±è´¥:`, error)

    // æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
    await updateDocumentStatus(document.id, 'failed', errorMessage)

    onProgress?.({
      documentId: document.id,
      status: 'failed',
      progress: 0,
      message: 'å¤„ç†å¤±è´¥',
      error: errorMessage,
    })

    return {
      success: false,
      documentId: document.id,
      processed: false,
      error: errorMessage,
    }
  }
}

/**
 * å¤„ç†æ–‡æ¡£çš„å®Œæ•´æµç¨‹ï¼ˆä½¿ç”¨å·²æå–çš„æ–‡æœ¬å†…å®¹ï¼‰
 *
 * æ–°çš„æ¨èæ–¹å¼ï¼šä¸Šä¼ æ—¶ç«‹å³è§£ææ–‡ä»¶ï¼Œç›´æ¥ä¼ é€’æ–‡æœ¬å†…å®¹
 * è·³è¿‡æ–‡ä»¶ä¸‹è½½å’Œè§£ææ­¥éª¤ï¼Œæé«˜æ•ˆç‡
 *
 * @param document - æ–‡æ¡£ä¿¡æ¯
 * @param extractedText - å·²æå–çš„æ–‡æœ¬å†…å®¹
 * @param options - å¤„ç†é€‰é¡¹
 * @param onProgress - è¿›åº¦å›è°ƒ
 */
export async function processDocumentWithText(
  document: Document,
  extractedText: string,
  options: ProcessingOptions = {},
  onProgress?: (progress: ProcessingProgress) => void
): Promise<ProcessingResult> {
  console.log(`ğŸ“„ [Processor] ä½¿ç”¨å·²æå–æ–‡æœ¬å¤„ç†æ–‡æ¡£: ${document.file_name} (${extractedText.length} å­—ç¬¦)`)

  // ç›´æ¥è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼Œèµ·å§‹è¿›åº¦ä¸º 0
  return processDocumentCore(document, extractedText, options, onProgress, 0)
}

/**
 * å¤„ç†æ–‡æ¡£çš„å®Œæ•´æµç¨‹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼Œéœ€è¦ä¸‹è½½æ–‡ä»¶ï¼‰
 *
 * @param document - æ–‡æ¡£ä¿¡æ¯
 * @param options - å¤„ç†é€‰é¡¹
 * @param onProgress - è¿›åº¦å›è°ƒ
 */
export async function processDocument(
  document: Document,
  options: ProcessingOptions = {},
  onProgress?: (progress: ProcessingProgress) => void
): Promise<ProcessingResult> {
  try {
    console.log(`ğŸš€ [Processor] Start processing document ${document.file_name} (docId=${document.id})`)

    // 1. Download file content
    onProgress?.({
      documentId: document.id,
      status: 'downloading',
      progress: 10,
      message: document.file_content ? 'Reading from local storage...' : 'Downloading from COS...',
    })

    let fileBuffer: Buffer
    if (document.file_content) {
      fileBuffer = base64ToBuffer(document.file_content)
    } else {
      fileBuffer = await downloadFileFromCOS(document.storage_path)
    }

    // 2. Parse file content
    onProgress?.({
      documentId: document.id,
      status: 'parsing',
      progress: 20,
      message: 'Parsing document content...',
    })

    const { content } = await parseFile(fileBuffer, document.file_name, document.mime_type)
    console.log(`âœ… [Processor] Parsed ${content.length} chars`)

    // Delegate to the unified core pipeline
    return await processDocumentCore(document, content, options, onProgress, 20)
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    incrementCounter('document_process_error')
    console.error('âŒ [Processor] Processing failed:', error)

    await updateDocumentStatus(document.id, 'failed', errorMessage)

    onProgress?.({
      documentId: document.id,
      status: 'failed',
      progress: 0,
      message: 'Processing failed',
      error: errorMessage,
    })

    return {
      success: false,
      documentId: document.id,
      processed: false,
      error: errorMessage,
    }
  }
}
async function parseFile(
  buffer: Buffer,
  fileName: string,
  mimeType?: string | null
): Promise<{ content: string; mimeType: string }> {
  const ext = fileName.split('.').pop()?.toLowerCase()
  const detectedMimeType = mimeType || detectMimeType(fileName)

  // æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£æå™¨
  if (ext === 'pdf' || detectedMimeType === 'application/pdf') {
    const result = await parsePDF(buffer.buffer as ArrayBuffer)
    return { content: result.content, mimeType: 'application/pdf' }
  }

  if (
    ext === 'docx' ||
    detectedMimeType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
  ) {
    const result = await parseDOCX(buffer)
    return { content: result.content, mimeType: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' }
  }

  // é»˜è®¤æŒ‰æ–‡æœ¬å¤„ç†
  const result = await parseTXT(buffer)
  return { content: result.content, mimeType: detectedMimeType || 'text/plain' }
}

/**
 * æ ¹æ®æ–‡ä»¶åæ£€æµ‹ MIME ç±»å‹
 */
function detectMimeType(fileName: string): string {
  const ext = fileName.split('.').pop()?.toLowerCase()

  const mimeMap: Record<string, string> = {
    pdf: 'application/pdf',
    docx: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    doc: 'application/msword',
    txt: 'text/plain',
    md: 'text/markdown',
    json: 'application/json',
  }

  return mimeMap[ext || ''] || 'text/plain'
}

// ==================== å¯¼å‡º ====================

/**
 * è§¦å‘æ–‡æ¡£å¤„ç†ï¼ˆä¾› API è°ƒç”¨ï¼‰
 *
 * è¿™ä¸ªå‡½æ•°è®¾è®¡ä¸ºå¼‚æ­¥è§¦å‘ï¼Œä¸é˜»å¡ API å“åº”
 */
export async function triggerDocumentProcessing(
  documentId: string,
  options?: ProcessingOptions
): Promise<{ documentId: string; status: string }> {
  // å¼‚æ­¥å¤„ç†ï¼Œä¸ç­‰å¾…å®Œæˆ
  processDocumentAsync(documentId, options).catch((error) => {
    console.error(`[Processor] å¼‚æ­¥å¤„ç†å¤±è´¥ (docId=${documentId}):`, error)
  })

  return {
    documentId,
    status: 'processing',
  }
}

/**
 * å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
 */
async function processDocumentAsync(
  documentId: string,
  options?: ProcessingOptions
): Promise<void> {
  // è¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¿é—®æ•°æ®åº“è·å–æ–‡æ¡£ä¿¡æ¯
  // å› ä¸ºè¿™ä¸ªå‡½æ•°æ˜¯å¼‚æ­¥è°ƒç”¨çš„
  // å®é™…å®ç°éœ€è¦åœ¨è°ƒç”¨æ–¹ä¼ å…¥å®Œæ•´æ–‡æ¡£ä¿¡æ¯
  // æˆ–è€…é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è·å–

  // ç®€åŒ–å®ç°ï¼šç”±è°ƒç”¨æ–¹è´Ÿè´£ä¼ å…¥å®Œæ•´ä¿¡æ¯
  console.log(`[Processor] å¼‚æ­¥å¤„ç†å·²è§¦å‘: docId=${documentId}`)
}
