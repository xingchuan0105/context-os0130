/**
 * æ–‡æ¡£å¤„ç†æµç¨‹
 *
 * å®Œæ•´çš„æ–‡æ¡£ä¸Šä¼ åå¤„ç†æµç¨‹ï¼? * 1. ä»?COS ä¸‹è½½æ–‡ä»¶
 * 2. è§£ææ–‡ä»¶å†…å®¹
 * 3. K-Type è®¤çŸ¥åˆ†æ
 * 4. çˆ¶å­åˆ†å—
 * 5. Embedding ç”Ÿæˆ
 * 6. ä¸‰å±‚å†™å…¥ Qdrant
 *
 * @module lib/processors/document-processor
 */

import COS from 'cos-nodejs-sdk-v5'
import { v4 as uuidv4 } from 'uuid'
import { processKTypeWorkflowEfficient, type KTypeProcessResult } from './k-type-efficient-vercel'
import { splitIntoParentChildChunksBatch } from '../chunkers/parent-child'
import embeddingClient from '../embedding'
import { ensureUserCollection, upsertPoints, type VectorPoint } from '../qdrant'
import {
  updateDocumentStatus,
  updateDocumentKType,
  type Document,
} from '../db/queries'
import { parsePDF } from '../parsers/pdf'
import { parseDOCX } from '../parsers/docx'
import { parseTXT } from '../parsers/text'
import { base64ToBuffer } from '../storage/local'

// ==================== é…ç½® ====================

const cos = new COS({
  SecretId: process.env.TENCENT_COS_SECRET_ID || '',
  SecretKey: process.env.TENCENT_COS_SECRET_KEY || '',
})

const BUCKET = process.env.TENCENT_COS_BUCKET || ''
const REGION = process.env.TENCENT_COS_REGION || 'ap-guangzhou'

// ==================== ç±»å‹å®šä¹‰ ====================

export interface ProcessingOptions {
  // K-Type åˆ†æé€‰é¡¹
  skipKType?: boolean

  // åˆ†å—é€‰é¡¹
  parentChunkSize?: number
  parentChunkOverlap?: number
  childChunkSize?: number
  childChunkOverlap?: number

  // Embedding é€‰é¡¹
  embeddingBatchSize?: number
}

export interface ProcessingResult {
  success: boolean
  documentId: string
  processed: boolean
  error?: string
  stats?: {
    textLength: number
    parentChunks: number
    childChunks: number
    embeddingTime: number
  }
}

export interface ProcessingProgress {
  documentId: string
  status: 'downloading' | 'parsing' | 'ktype' | 'chunking' | 'embedding' | 'qdrant' | 'completed' | 'failed'
  progress: number // 0-100
  message: string
  error?: string
}

// ==================== ä¸»å¤„ç†æµç¨?====================

/**
 * æ ¸å¿ƒæ–‡æ¡£å¤„ç†æµç¨‹ï¼ˆç»Ÿä¸€çš„å†…éƒ¨å®ç°ï¼‰
 *
 * è¿™ä¸ªå‡½æ•°åŒ…å«äº†æ‰€æœ‰æ–‡æ¡£å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤? * ä¸¤ä¸ªå…¬å…±æ¥å£ processDocument å’?processDocumentWithText éƒ½è°ƒç”¨è¿™ä¸ªå‡½æ•? *
 * @param document - æ–‡æ¡£ä¿¡æ¯
 * @param textContent - æ–‡æœ¬å†…å®¹ï¼ˆå·²æå–ï¼? * @param options - å¤„ç†é€‰é¡¹
 * @param onProgress - è¿›åº¦å›è°ƒ
 * @param startProgress - èµ·å§‹è¿›åº¦å€¼ï¼ˆç”¨äºä¸åŒå…¥å£çš„è¿›åº¦è°ƒæ•´ï¼‰
 * @returns å¤„ç†ç»“æœ
 */
async function processDocumentCore(
  document: Document,
  textContent: string,
  options: ProcessingOptions,
  onProgress?: (progress: ProcessingProgress) => void,
  startProgress = 0
): Promise<ProcessingResult> {
  const {
    skipKType = false,
    // ä¸åˆ†å—å™¨é»˜è®¤ä¿æŒä¸€è‡´ï¼šæ”¾å¤§å—å¹¶æé«˜é‡å ï¼Œæå‡å¬å›è¦†ç›–ç‡
    parentChunkSize = 1600,
    parentChunkOverlap = 240,
    childChunkSize = 420,
    childChunkOverlap = 120,
    embeddingBatchSize = 10,
  } = options

  try {
    console.log(`ğŸ“„ [Processor] å¼€å§‹å¤„ç†æ–‡æ¡? ${document.file_name} (docId=${document.id})`)
    console.log(`ğŸ“¥ [Processor] æ–‡æœ¬å†…å®¹é•¿åº¦: ${textContent.length} å­—ç¬¦`)

    // 1. K-Type è®¤çŸ¥åˆ†æ
    let ktypeResult: KTypeProcessResult | null = null
    if (!skipKType) {
      onProgress?.({
        documentId: document.id,
        status: 'ktype',
        progress: startProgress + 20,
        message: 'K-Type è®¤çŸ¥åˆ†æä¸?..',
      })

      try {
        ktypeResult = await processKTypeWorkflowEfficient(textContent)
        console.log(`ğŸ§  [Processor] K-Type åˆ†æå®Œæˆ`)
        console.log(`   ä¸»å¯¼ç±»å‹: ${ktypeResult.finalReport.classification.dominantType.join(', ')}`)
        console.log(`   çŸ¥è¯†æ¨¡å—: ${ktypeResult.finalReport.knowledgeModules.length} ä¸ª`)
      } catch (error) {
        console.warn(`âš ï¸  [Processor] K-Type åˆ†æå¤±è´¥ï¼Œè·³è¿?`, error)
      }
    }

    // 2. çˆ¶å­åˆ†å—
    onProgress?.({
      documentId: document.id,
      status: 'chunking',
      progress: startProgress + 40,
      message: 'æ™ºèƒ½åˆ†å—å¤„ç†...',
    })

    const { parentChunks, childChunks } = await splitIntoParentChildChunksBatch(textContent, {
      parentChunkSize,
      parentChunkOverlap,
      childChunkSize,
      childChunkOverlap,
    })

    console.log(`âœ‚ï¸  [Processor] åˆ†å—å®Œæˆ: ${parentChunks.length} ä¸ªçˆ¶å? ${childChunks.length} ä¸ªå­å—`)

    // 3. Embedding + Qdrant å†™å…¥
    onProgress?.({
      documentId: document.id,
      status: 'embedding',
      progress: startProgress + 50,
      message: 'ç”Ÿæˆå‘é‡å¹¶å†™å…¥æ•°æ®åº“...',
    })

    const embeddingStartTime = Date.now()

    // 3.1 å‡†å¤‡æ‰€æœ‰æ–‡æœ¬å’Œå‘é‡ç‚?    const points: VectorPoint[] = []
    const textsToEmbed: string[] = []

    // 3.1.1 æ·»åŠ æ–‡æ¡£å±?(K-Type æ‘˜è¦)
    if (ktypeResult) {
      textsToEmbed.push(ktypeResult.finalReport.executiveSummary)
    }

    // 3.1.2 æ·»åŠ çˆ¶å—
    for (const parentChunk of parentChunks) {
      textsToEmbed.push(parentChunk.content)
    }

    // 3.1.3 æ·»åŠ å­å—
    for (const childChunk of childChunks) {
      textsToEmbed.push(childChunk.content)
    }

    // 3.2 æ‰¹é‡ç”Ÿæˆå‘é‡
    const allEmbeddings: number[][] = []
    for (let i = 0; i < textsToEmbed.length; i += embeddingBatchSize) {
      const batch = textsToEmbed.slice(i, i + embeddingBatchSize)

      const response = await embeddingClient.embeddings.create({
        model: process.env.EMBEDDING_MODEL || 'qwen3-embedding-4b',
        input: batch,
      })

      allEmbeddings.push(...response.data.map(d => d.embedding))

      // æ›´æ–°è¿›åº¦
      const progressIncrement = Math.floor(((i + batch.length) / textsToEmbed.length) * 40)
      onProgress?.({
        documentId: document.id,
        status: 'embedding',
        progress: startProgress + 50 + progressIncrement,
        message: `å‘é‡ç”Ÿæˆè¿›åº¦: ${i + batch.length}/${textsToEmbed.length}`,
      })
    }

    let embedIndex = 0

    // 3.3 åˆ›å»ºæ–‡æ¡£å±‚å‘é‡ç‚¹
    if (ktypeResult) {
      const docVector = allEmbeddings[embedIndex++]
      points.push({
        id: uuidv4(),
        vector: docVector,
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'document',
          content: ktypeResult.finalReport.executiveSummary,
          chunk_index: 0,
          metadata: {
            file_name: document.file_name,
            ktype: {
              dominant_type: ktypeResult.finalReport.classification.dominantType[0] || 'unknown',
              dominant_types: ktypeResult.finalReport.classification.dominantType,
              type_scores: ktypeResult.finalReport.classification.scores,
              knowledge_modules: ktypeResult.finalReport.knowledgeModules.map(m => m.type),
              dikw_level: ktypeResult.finalReport.scanTrace.dikwLevel,
              logic_pattern: ktypeResult.finalReport.scanTrace.logicPattern,
            },
          },
        },
      })
      console.log(`ğŸ“ [Processor] æ–‡æ¡£å±‚å‘é‡å·²å‡†å¤‡`)
    }

    // 3.4 åˆ›å»ºçˆ¶å—å‘é‡ç‚?    for (const parentChunk of parentChunks) {
      const parentId = uuidv4()
      points.push({
        id: parentId,
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'parent',
          content: parentChunk.content,
          chunk_index: parentChunk.index,
          metadata: {
            file_name: document.file_name,
          },
        },
      })
    }

    // 3.5 åˆ›å»ºå­å—å‘é‡ç‚?    for (const childChunk of childChunks) {
      const childId = uuidv4()
      points.push({
        id: childId,
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'child',
          parent_id: `parent_${document.id}_${childChunk.parentIndex}`,
          content: childChunk.content,
          chunk_index: childChunk.index,
          metadata: {
            file_name: document.file_name,
            parent_index: childChunk.parentIndex,
          },
        },
      })
    }

    // 3.6 æ‰¹é‡å†™å…¥ Qdrant
    await ensureUserCollection(document.user_id)
    await upsertPoints(document.user_id, points)

    const embeddingTime = Date.now() - embeddingStartTime
    console.log(`âœ?[Processor] å‘é‡åŒ–å®Œæˆ? è€—æ—¶ ${(embeddingTime / 1000).toFixed(2)}s, ${points.length} ä¸ªå‘é‡ç‚¹`)

    // 4. æ›´æ–°æ•°æ®åº?    await updateDocumentKType(
      document.id,
      JSON.stringify(ktypeResult?.finalReport.classification || {}),
      JSON.stringify(ktypeResult?.finalReport || {}),
      ktypeResult?.finalReport.distilledContent || '',
      childChunks.length
    )

    onProgress?.({
      documentId: document.id,
      status: 'completed',
      progress: 100,
      message: 'å¤„ç†å®Œæˆ',
    })

    console.log(`âœ?[Processor] æ–‡æ¡£å¤„ç†å®Œæˆ: ${document.file_name}`)

    return {
      success: true,
      documentId: document.id,
      processed: true,
      stats: {
        textLength: textContent.length,
        parentChunks: parentChunks.length,
        childChunks: childChunks.length,
        embeddingTime,
      },
    }
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    console.error(`â?[Processor] å¤„ç†å¤±è´¥:`, error)

    // æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
    await updateDocumentStatus(document.id, 'failed', errorMessage)

    onProgress?.({
      documentId: document.id,
      status: 'failed',
      progress: 0,
      message: 'å¤„ç†å¤±è´¥',
      error: errorMessage,
    })

    return {
      success: false,
      documentId: document.id,
      processed: false,
      error: errorMessage,
    }
  }
}

/**
 * å¤„ç†æ–‡æ¡£çš„å®Œæ•´æµç¨‹ï¼ˆä½¿ç”¨å·²æå–çš„æ–‡æœ¬å†…å®¹ï¼? *
 * æ–°çš„æ¨èæ–¹å¼ï¼šä¸Šä¼ æ—¶ç«‹å³è§£ææ–‡ä»¶ï¼Œç›´æ¥ä¼ é€’æ–‡æœ¬å†…å®? * è·³è¿‡æ–‡ä»¶ä¸‹è½½å’Œè§£ææ­¥éª¤ï¼Œæé«˜æ•ˆç‡
 *
 * @param document - æ–‡æ¡£ä¿¡æ¯
 * @param extractedText - å·²æå–çš„æ–‡æœ¬å†…å®¹
 * @param options - å¤„ç†é€‰é¡¹
 * @param onProgress - è¿›åº¦å›è°ƒ
 */
export async function processDocumentWithText(
  document: Document,
  extractedText: string,
  options: ProcessingOptions = {},
  onProgress?: (progress: ProcessingProgress) => void
): Promise<ProcessingResult> {
  console.log(`ğŸ“„ [Processor] ä½¿ç”¨å·²æå–æ–‡æœ¬å¤„ç†æ–‡æ¡? ${document.file_name} (${extractedText.length} å­—ç¬¦)`)

  // ç›´æ¥è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼Œèµ·å§‹è¿›åº¦ä¸º 0
  return processDocumentCore(document, extractedText, options, onProgress, 0)
}

/**
 * å¤„ç†æ–‡æ¡£çš„å®Œæ•´æµç¨‹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼Œéœ€è¦ä¸‹è½½æ–‡ä»¶ï¼‰
 *
 * @param document - æ–‡æ¡£ä¿¡æ¯
 * @param options - å¤„ç†é€‰é¡¹
 * @param onProgress - è¿›åº¦å›è°ƒ
 */
export async function processDocument(
  document: Document,
  options: ProcessingOptions = {},
  onProgress?: (progress: ProcessingProgress) => void
): Promise<ProcessingResult> {
  const {
    skipKType = false,
    parentChunkSize = 1600,
    parentChunkOverlap = 240,
    childChunkSize = 420,
    childChunkOverlap = 120,
    embeddingBatchSize = 10,
  } = options

  try {
    console.log(`ğŸ“„ [Processor] å¼€å§‹å¤„ç†æ–‡æ¡? ${document.file_name} (docId=${document.id})`)

    // 1. è·å–æ–‡ä»¶å†…å®¹ï¼ˆä»æœ¬åœ°å­˜å‚¨æˆ–COSï¼?    onProgress?.({
      documentId: document.id,
      status: 'downloading',
      progress: 10,
      message: document.file_content ? 'ä»æœ¬åœ°å­˜å‚¨è¯»å–æ–‡ä»?..' : 'ä»è…¾è®¯äº‘COSä¸‹è½½æ–‡ä»¶...',
    })

    let fileBuffer: Buffer
    if (document.file_content) {
      // ä»æœ¬åœ°å­˜å‚¨è¯»å–ï¼ˆbase64ï¼?      console.log(`ğŸ“¥ [Processor] ä»æœ¬åœ°å­˜å‚¨è¯»å–æ–‡ä»¶`)
      fileBuffer = base64ToBuffer(document.file_content)
    } else {
      // ä»COSä¸‹è½½
      fileBuffer = await downloadFileFromCOS(document.storage_path)
    }

    console.log(`ğŸ“¥ [Processor] æ–‡ä»¶è¯»å–å®Œæˆ: ${(fileBuffer.length / 1024).toFixed(2)} KB`)

    // 2. è§£ææ–‡ä»¶å†…å®¹
    onProgress?.({
      documentId: document.id,
      status: 'parsing',
      progress: 20,
      message: 'è§£ææ–‡ä»¶å†…å®¹...',
    })

    const { content, mimeType } = await parseFile(fileBuffer, document.file_name, document.mime_type)
    console.log(`ğŸ“– [Processor] æ–‡ä»¶è§£æå®Œæˆ: ${content.length} å­—ç¬¦`)

    // 3. K-Type è®¤çŸ¥åˆ†æ
    let ktypeResult: KTypeProcessResult | null = null
    if (!skipKType) {
      onProgress?.({
        documentId: document.id,
        status: 'ktype',
        progress: 30,
        message: 'K-Type è®¤çŸ¥åˆ†æä¸?..',
      })

      try {
        ktypeResult = await processKTypeWorkflowEfficient(content)
        console.log(`ğŸ§  [Processor] K-Type åˆ†æå®Œæˆ`)
        console.log(`   ä¸»å¯¼ç±»å‹: ${ktypeResult.finalReport.classification.dominantType.join(', ')}`)
        console.log(`   çŸ¥è¯†æ¨¡å—: ${ktypeResult.finalReport.knowledgeModules.length} ä¸ª`)
      } catch (error) {
        console.warn(`âš ï¸  [Processor] K-Type åˆ†æå¤±è´¥ï¼Œè·³è¿?`, error)
      }
    }

    // 4. çˆ¶å­åˆ†å—
    onProgress?.({
      documentId: document.id,
      status: 'chunking',
      progress: 50,
      message: 'æ™ºèƒ½åˆ†å—å¤„ç†...',
    })

    const { parentChunks, childChunks } = await splitIntoParentChildChunksBatch(content, {
      parentChunkSize,
      parentChunkOverlap,
      childChunkSize,
      childChunkOverlap,
    })

    console.log(`âœ‚ï¸  [Processor] åˆ†å—å®Œæˆ: ${parentChunks.length} ä¸ªçˆ¶å? ${childChunks.length} ä¸ªå­å—`)

    // 5. Embedding + Qdrant å†™å…¥
    onProgress?.({
      documentId: document.id,
      status: 'embedding',
      progress: 60,
      message: 'ç”Ÿæˆå‘é‡å¹¶å†™å…¥æ•°æ®åº“...',
    })

    const embeddingStartTime = Date.now()

    // 5.1 å‡†å¤‡æ‰€æœ‰æ–‡æœ¬å’Œå‘é‡ç‚?    const points: VectorPoint[] = []
    const textsToEmbed: string[] = []

    // 5.1.1 æ·»åŠ æ–‡æ¡£å±?(K-Type æ‘˜è¦)
    if (ktypeResult) {
      textsToEmbed.push(ktypeResult.finalReport.executiveSummary)
    }

    // 5.1.2 æ·»åŠ çˆ¶å—
    for (const parentChunk of parentChunks) {
      textsToEmbed.push(parentChunk.content)
    }

    // 5.1.3 æ·»åŠ å­å—
    for (const childChunk of childChunks) {
      textsToEmbed.push(childChunk.content)
    }

    // 5.2 æ‰¹é‡ç”Ÿæˆå‘é‡
    const allEmbeddings: number[][] = []
    for (let i = 0; i < textsToEmbed.length; i += embeddingBatchSize) {
      const batch = textsToEmbed.slice(i, i + embeddingBatchSize)

      const response = await embeddingClient.embeddings.create({
        model: process.env.EMBEDDING_MODEL || 'qwen3-embedding-4b',
        input: batch,
      })

      allEmbeddings.push(...response.data.map(d => d.embedding))

      // æ›´æ–°è¿›åº¦
      onProgress?.({
        documentId: document.id,
        status: 'embedding',
        progress: 60 + Math.floor(((i + batch.length) / textsToEmbed.length) * 30),
        message: `å‘é‡ç”Ÿæˆè¿›åº¦: ${i + batch.length}/${textsToEmbed.length}`,
      })
    }

    let embedIndex = 0

    // 5.3 åˆ›å»ºæ–‡æ¡£å±‚å‘é‡ç‚¹
    if (ktypeResult) {
      const docVector = allEmbeddings[embedIndex++]
      points.push({
        id: uuidv4(),
        vector: docVector,
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'document',
          content: ktypeResult.finalReport.executiveSummary,
          chunk_index: 0,
          metadata: {
            file_name: document.file_name,
            ktype: {
              dominant_type: ktypeResult.finalReport.classification.dominantType[0] || 'unknown',
              dominant_types: ktypeResult.finalReport.classification.dominantType,
              type_scores: ktypeResult.finalReport.classification.scores,
              knowledge_modules: ktypeResult.finalReport.knowledgeModules.map(m => m.type),
              dikw_level: ktypeResult.finalReport.scanTrace.dikwLevel,
              logic_pattern: ktypeResult.finalReport.scanTrace.logicPattern,
            },
          },
        },
      })
      console.log(`ğŸ“ [Processor] æ–‡æ¡£å±‚å‘é‡å·²å‡†å¤‡`)
    }

    // 5.4 åˆ›å»ºçˆ¶å—å‘é‡ç‚?    for (const parentChunk of parentChunks) {
      const parentId = uuidv4()
      points.push({
        id: parentId,
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'parent',
          content: parentChunk.content,
          chunk_index: parentChunk.index,
          metadata: {
            file_name: document.file_name,
          },
        },
      })
    }

    // 5.5 åˆ›å»ºå­å—å‘é‡ç‚?    for (const childChunk of childChunks) {
      const childId = uuidv4()
      points.push({
        id: childId,
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id: document.id,
          kb_id: document.kb_id,
          user_id: document.user_id,
          type: 'child',
          parent_id: `parent_${document.id}_${childChunk.parentIndex}`,
          content: childChunk.content,
          chunk_index: childChunk.index,
          metadata: {
            file_name: document.file_name,
            parent_index: childChunk.parentIndex,
          },
        },
      })
    }

    // 5.6 æ‰¹é‡å†™å…¥ Qdrant
    await ensureUserCollection(document.user_id)
    await upsertPoints(document.user_id, points)

    const embeddingTime = Date.now() - embeddingStartTime
    console.log(`âœ?[Processor] å‘é‡åŒ–å®Œæˆ? è€—æ—¶ ${(embeddingTime / 1000).toFixed(2)}s, ${points.length} ä¸ªå‘é‡ç‚¹`)

    // 6. æ›´æ–°æ•°æ®åº?    await updateDocumentKType(
      document.id,
      JSON.stringify(ktypeResult?.finalReport.classification || {}),
      JSON.stringify(ktypeResult?.finalReport || {}),
      ktypeResult?.finalReport.distilledContent || '',
      childChunks.length
    )

    onProgress?.({
      documentId: document.id,
      status: 'completed',
      progress: 100,
      message: 'å¤„ç†å®Œæˆ',
    })

    console.log(`âœ?[Processor] æ–‡æ¡£å¤„ç†å®Œæˆ: ${document.file_name}`)

    return {
      success: true,
      documentId: document.id,
      processed: true,
      stats: {
        textLength: content.length,
        parentChunks: parentChunks.length,
        childChunks: childChunks.length,
        embeddingTime,
      },
    }
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    console.error(`â?[Processor] å¤„ç†å¤±è´¥:`, error)

    // æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
    await updateDocumentStatus(document.id, 'failed', errorMessage)

    onProgress?.({
      documentId: document.id,
      status: 'failed',
      progress: 0,
      message: 'å¤„ç†å¤±è´¥',
      error: errorMessage,
    })

    return {
      success: false,
      documentId: document.id,
      processed: false,
      error: errorMessage,
    }
  }
}

// ==================== è¾…åŠ©å‡½æ•° ====================

/**
 * ä»?COS ä¸‹è½½æ–‡ä»¶
 */
async function downloadFileFromCOS(cosKey: string): Promise<Buffer> {
  return new Promise((resolve, reject) => {
    cos.getObject(
      {
        Bucket: BUCKET,
        Region: REGION,
        Key: cosKey,
      },
      (err, data) => {
        if (err) {
          console.error('COS download error:', err)
          reject(new Error(`æ–‡ä»¶ä¸‹è½½å¤±è´¥: ${err.message}`))
        } else {
          resolve(data.Body as Buffer)
        }
      }
    )
  })
}

/**
 * è§£ææ–‡ä»¶å†…å®¹
 */
async function parseFile(
  buffer: Buffer,
  fileName: string,
  mimeType?: string | null
): Promise<{ content: string; mimeType: string }> {
  const ext = fileName.split('.').pop()?.toLowerCase()
  const detectedMimeType = mimeType || detectMimeType(fileName)

  // æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£æå™?  if (ext === 'pdf' || detectedMimeType === 'application/pdf') {
    const result = await parsePDF(buffer.buffer as ArrayBuffer)
    return { content: result.content, mimeType: 'application/pdf' }
  }

  if (
    ext === 'docx' ||
    detectedMimeType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
  ) {
    const result = await parseDOCX(buffer)
    return { content: result.content, mimeType: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' }
  }

  // é»˜è®¤æŒ‰æ–‡æœ¬å¤„ç?  const result = await parseTXT(buffer)
  return { content: result.content, mimeType: detectedMimeType || 'text/plain' }
}

/**
 * æ ¹æ®æ–‡ä»¶åæ£€æµ?MIME ç±»å‹
 */
function detectMimeType(fileName: string): string {
  const ext = fileName.split('.').pop()?.toLowerCase()

  const mimeMap: Record<string, string> = {
    pdf: 'application/pdf',
    docx: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    doc: 'application/msword',
    txt: 'text/plain',
    md: 'text/markdown',
    json: 'application/json',
  }

  return mimeMap[ext || ''] || 'text/plain'
}

// ==================== å¯¼å‡º ====================

/**
 * è§¦å‘æ–‡æ¡£å¤„ç†ï¼ˆä¾› API è°ƒç”¨ï¼? *
 * è¿™ä¸ªå‡½æ•°è®¾è®¡ä¸ºå¼‚æ­¥è§¦å‘ï¼Œä¸é˜»å¡?API å“åº”
 */
export async function triggerDocumentProcessing(
  documentId: string,
  options?: ProcessingOptions
): Promise<{ documentId: string; status: string }> {
  // å¼‚æ­¥å¤„ç†ï¼Œä¸ç­‰å¾…å®Œæˆ
  processDocumentAsync(documentId, options).catch((error) => {
    console.error(`[Processor] å¼‚æ­¥å¤„ç†å¤±è´¥ (docId=${documentId}):`, error)
  })

  return {
    documentId,
    status: 'processing',
  }
}

/**
 * å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
 */
async function processDocumentAsync(
  documentId: string,
  options?: ProcessingOptions
): Promise<void> {
  // è¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¿é—®æ•°æ®åº“è·å–æ–‡æ¡£ä¿¡æ?  // å› ä¸ºè¿™ä¸ªå‡½æ•°æ˜¯å¼‚æ­¥è°ƒç”¨çš„
  // å®é™…å®ç°éœ€è¦åœ¨è°ƒç”¨æ–¹ä¼ å…¥å®Œæ•´æ–‡æ¡£ä¿¡æ?  // æˆ–è€…é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è·å?
  // ç®€åŒ–å®ç°ï¼šç”±è°ƒç”¨æ–¹è´Ÿè´£ä¼ å…¥å®Œæ•´ä¿¡æ¯
  console.log(`[Processor] å¼‚æ­¥å¤„ç†å·²è§¦å? docId=${documentId}`)
}
