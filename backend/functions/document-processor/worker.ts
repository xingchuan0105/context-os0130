/**
 * æ–‡æ¡£å¤„ç† Worker (å‡½æ•°è®¡ç®—ç‰ˆæœ¬)
 *
 * ä» lib/worker-three-layer.ts æå–çš„æ ¸å¿ƒå¤„ç†é€»è¾‘
 * é€‚é…å‡½æ•°è®¡ç®—ç¯å¢ƒ (æ—  BullMQ ä¾èµ–)
 */

import { createClient } from '@supabase/supabase-js'
import embeddingClient from './embedding'
import { parseFile, parseWebPage as parseWebPageContent } from './parsers'
import { splitIntoParentChildChunksBatch } from './chunkers'
import { processKTypeWorkflowWithFallback } from './processors/k-type'
import { buildKTypeSummaryText, buildKTypeMetadata } from './processors/k-type-summary'
import {
  ensureUserCollection,
  batchUpsert,
  deleteDocumentChunks,
  type VectorPoint,
} from './qdrant'

// ==================== é…ç½® ====================

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_KEY!
)

// ==================== ç±»å‹å®šä¹‰ ====================

interface IngestJobData {
  doc_id: string
  storage_path: string
  kb_id: string
  user_id: string
}

interface ProcessResult {
  success: boolean
  chunks_count: number
  doc_index?: number
}

// ==================== å·¥å…·å‡½æ•° ====================

function logMemoryUsage(stage: string): void {
  const usage = process.memoryUsage()
  console.log(
    `ğŸ’¾ [MEM] ${stage}: RSS=${Math.round(usage.rss / 1024 / 1024)}MB, ` +
    `Heap=${Math.round(usage.heapUsed / 1024 / 1024)}MB`
  )
}

async function updateDocStatus(
  docId: string,
  status: string,
  extra?: object
): Promise<void> {
  await supabase.from('documents').update({ status, ...extra }).eq('id', docId)
}

// ==================== ID åˆ†é…ç­–ç•¥ ====================

class QdrantIdGenerator {
  private docIndex: number
  private readonly DOCUMENT_BASE = 1_000_000
  private readonly PARENT_MULTIPLIER = 10_000
  private readonly CHILD_MULTIPLIER = 100

  constructor(docIndex: number) {
    this.docIndex = docIndex
  }

  getDocumentId(): number {
    return this.DOCUMENT_BASE + this.docIndex
  }

  getParentId(parentIndex: number): number {
    return this.docIndex * this.PARENT_MULTIPLIER + parentIndex
  }

  getChildId(parentIndex: number, childIndex: number): number {
    return this.docIndex * this.PARENT_MULTIPLIER + parentIndex * this.CHILD_MULTIPLIER + childIndex
  }
}

// ==================== ä¸»å¤„ç†å‡½æ•° ====================

export async function processDocument(jobData: IngestJobData): Promise<ProcessResult> {
  const { doc_id, storage_path, kb_id, user_id } = jobData

  logMemoryUsage('processDocument-start')
  await updateDocStatus(doc_id, 'processing')

  try {
    // 1. ä» Supabase Storage ä¸‹è½½æ–‡ä»¶
    const { data: fileData, error: downloadError } = await supabase.storage
      .from('documents')
      .download(storage_path)

    if (downloadError || !fileData) {
      throw new Error(`ä¸‹è½½å¤±è´¥: ${downloadError?.message}`)
    }

    const buffer = Buffer.from(await fileData.arrayBuffer())
    const fileName = storage_path.split('/').pop() || 'unknown'

    // 2. è§£ææ–‡ä»¶å†…å®¹
    let text = ''
    const mimeType = fileData.type || ''

    const parsed = await parseFile(buffer, mimeType, fileName)
    text = parsed.content

    // é‡Šæ”¾ buffer
    ;(buffer as any).fill?.(0)

    if (!text.trim()) {
      throw new Error('æ–‡æ¡£å†…å®¹ä¸ºç©º')
    }

    logMemoryUsage('after-parsing')

    // 3. K-Type åˆ†æ (å¯¹æ•´ç¯‡åŸæ–‡)
    console.log(`ğŸ” [K-Type] å¼€å§‹åˆ†ææ–‡æ¡£...`)
    const ktypeResult = await processKTypeWorkflowWithFallback(text)

    // ç”Ÿæˆ K-Type æ‘˜è¦æ–‡æœ¬ (ç”¨äºæ–‡æ¡£çº§å‘é‡)
    const ktypeSummary = buildKTypeSummaryText(ktypeResult)

    // ç”Ÿæˆ K-Type å…ƒæ•°æ®
    const ktypeMetadata = buildKTypeMetadata(ktypeResult)

    console.log(`ğŸ“Š [K-Type] ä¸»å¯¼ç±»å‹: ${ktypeMetadata.dominant_type}`)
    console.log(`ğŸ“ [K-Type] æ‘˜è¦é•¿åº¦: ${ktypeSummary.length} å­—ç¬¦`)

    logMemoryUsage('after-ktype')

    // 4. çˆ¶å­åˆ†å—
    const { parentChunks, childChunks } = await splitIntoParentChildChunksBatch(text, {
      parentChunkSize: 1024,
      childChunkSize: 256,
      removeExtraSpaces: true,
      removeUrlsEmails: true,
    })

    console.log(`ğŸ“¦ [Chunk] çˆ¶å—: ${parentChunks.length}, å­å—: ${childChunks.length}`)
    logMemoryUsage('after-chunking')

    // 5. ç¡®ä¿ Qdrant collection å­˜åœ¨
    const collectionName = await ensureUserCollection(user_id)
    console.log(`ğŸ“¦ [Qdrant] ä½¿ç”¨ collection: ${collectionName}`)

    // 6. å‡†å¤‡ä¸‰å±‚åµŒå…¥å†…å®¹
    const docIndex = Date.now() % 10000
    const idGen = new QdrantIdGenerator(docIndex)

    const textsToEmbed: string[] = [
      ktypeSummary,
      ...parentChunks.map(p => p.content),
      ...childChunks.map(c => c.content),
    ]

    console.log(`ğŸ”„ [Embed] å‡†å¤‡åµŒå…¥ ${textsToEmbed.length} ä¸ªæ–‡æœ¬å—`)

    // 7. æ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥
    const embeddingModel = process.env.EMBEDDING_MODEL || 'BAAI/bge-m3'
    const batchSize = parseInt(process.env.EMBEDDING_BATCH_SIZE || '50')

    const allEmbeddings: number[][] = []

    for (let i = 0; i < textsToEmbed.length; i += batchSize) {
      const batch = textsToEmbed.slice(i, i + batchSize)

      const embeddingResponse = await embeddingClient.embeddings.create({
        model: embeddingModel,
        input: batch,
      })

      allEmbeddings.push(...embeddingResponse.data.map(d => d.embedding))
      console.log(`âœ… æ‰¹æ¬¡ ${Math.floor(i / batchSize) + 1}: ${embeddingResponse.data.length} ä¸ªå‘é‡`)
    }

    // 8. å‡†å¤‡ Qdrant å‘é‡ç‚¹
    const points: VectorPoint[] = []
    let embedIndex = 0

    // 8.1 æ–‡æ¡£çº§å‘é‡ç‚¹
    points.push({
      id: idGen.getDocumentId(),
      vector: allEmbeddings[embedIndex++],
      payload: {
        doc_id,
        kb_id,
        user_id,
        type: 'document',
        content: ktypeSummary,
        chunk_index: 0,
        metadata: {
          ktype: ktypeMetadata,
        },
      },
    })

    // 8.2 çˆ¶å—å‘é‡ç‚¹
    for (const parent of parentChunks) {
      points.push({
        id: idGen.getParentId(parent.index),
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id,
          kb_id,
          user_id,
          type: 'parent',
          content: parent.content,
          chunk_index: parent.index,
          metadata: {
            file_name: fileName,
          },
        },
      })
    }

    // 8.3 å­å—å‘é‡ç‚¹
    for (const child of childChunks) {
      const parentQdrantId = idGen.getParentId(child.parentIndex)

      points.push({
        id: idGen.getChildId(child.parentIndex, child.index),
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id,
          kb_id,
          user_id,
          type: 'child',
          parent_id: parentQdrantId,
          content: child.content,
          chunk_index: child.index,
          metadata: {
            file_name: fileName,
            parent_index: child.parentIndex,
          },
        },
      })
    }

    console.log(`ğŸ“¦ [Qdrant] å‡†å¤‡äº† ${points.length} ä¸ªå‘é‡ç‚¹`)

    // 9. æ‰¹é‡æ’å…¥ Qdrant
    await batchUpsert(user_id, points, batchSize)
    console.log(`âœ… [Qdrant] æˆåŠŸæ’å…¥ ${points.length} ä¸ªå‘é‡ç‚¹`)

    // æ¸…ç†
    ;(textsToEmbed as any).length = 0
    ;(allEmbeddings as any).length = 0
    ;(points as any).length = 0

    if (global.gc) {
      global.gc()
    }

    logMemoryUsage('after-embedding')

    // 10. æ›´æ–°æ•°æ®åº“è®°å½•
    await updateDocStatus(doc_id, 'completed', {
      deep_summary: ktypeResult.finalReport,
      ktype_summary: ktypeSummary,
      ktype_metadata: ktypeMetadata as any,
      chunk_count: parentChunks.length + childChunks.length,
    })

    return {
      success: true,
      chunks_count: points.length,
      doc_index: docIndex,
    }
  } catch (error: any) {
    const message = error.message || 'æœªçŸ¥é”™è¯¯'
    console.error('âŒ [ERROR] æ–‡æ¡£å¤„ç†å¤±è´¥!')
    console.error('  - doc_id:', doc_id)
    console.error('  - error:', message)

    await updateDocStatus(doc_id, 'failed', { error_message: message })

    // æ¸…ç† Qdrant ä¸­çš„æ•°æ®
    try {
      await deleteDocumentChunks(user_id, doc_id)
      console.log(`ğŸ§¹ [Qdrant] å·²æ¸…ç†æ–‡æ¡£ ${doc_id} çš„å‘é‡æ•°æ®`)
    } catch (e) {
      console.error('æ¸…ç† Qdrant æ•°æ®å¤±è´¥:', e)
    }

    throw error
  }
}

/**
 * å¤„ç†ç½‘é¡µå†…å®¹
 */
export async function processWebPage(jobData: IngestJobData): Promise<ProcessResult> {
  const { doc_id, storage_path: url, kb_id, user_id } = jobData

  await updateDocStatus(doc_id, 'processing')

  try {
    // è·å–ç½‘é¡µå†…å®¹
    const { content } = await parseWebPageContent(url, { method: 'jina' })

    if (!content.trim()) {
      throw new Error('ç½‘é¡µå†…å®¹ä¸ºç©º')
    }

    // K-Type åˆ†æ
    const ktypeResult = await processKTypeWorkflowWithFallback(content)
    const ktypeSummary = buildKTypeSummaryText(ktypeResult)
    const ktypeMetadata = buildKTypeMetadata(ktypeResult)

    // çˆ¶å­åˆ†å—
    const { parentChunks, childChunks } = await splitIntoParentChildChunksBatch(content, {
      parentChunkSize: 1024,
      childChunkSize: 256,
      removeExtraSpaces: true,
      removeUrlsEmails: true,
    })

    await ensureUserCollection(user_id)

    const docIndex = Date.now() % 10000
    const idGen = new QdrantIdGenerator(docIndex)

    const textsToEmbed: string[] = [
      ktypeSummary,
      ...parentChunks.map(p => p.content),
      ...childChunks.map(c => c.content),
    ]

    const embeddingModel = process.env.EMBEDDING_MODEL || 'BAAI/bge-m3'
    const batchSize = parseInt(process.env.EMBEDDING_BATCH_SIZE || '50')

    const allEmbeddings: number[][] = []

    for (let i = 0; i < textsToEmbed.length; i += batchSize) {
      const batch = textsToEmbed.slice(i, i + batchSize)

      const embeddingResponse = await embeddingClient.embeddings.create({
        model: embeddingModel,
        input: batch,
      })

      allEmbeddings.push(...embeddingResponse.data.map(d => d.embedding))
    }

    const points: VectorPoint[] = []
    let embedIndex = 0

    // Document
    points.push({
      id: idGen.getDocumentId(),
      vector: allEmbeddings[embedIndex++],
      payload: {
        doc_id,
        kb_id,
        user_id,
        type: 'document',
        content: ktypeSummary,
        chunk_index: 0,
        metadata: { ktype: ktypeMetadata, source_url: url },
      },
    })

    // Parents
    for (const parent of parentChunks) {
      points.push({
        id: idGen.getParentId(parent.index),
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id,
          kb_id,
          user_id,
          type: 'parent',
          content: parent.content,
          chunk_index: parent.index,
          metadata: { source_url: url },
        },
      })
    }

    // Children
    for (const child of childChunks) {
      const parentQdrantId = idGen.getParentId(child.parentIndex)
      points.push({
        id: idGen.getChildId(child.parentIndex, child.index),
        vector: allEmbeddings[embedIndex++],
        payload: {
          doc_id,
          kb_id,
          user_id,
          type: 'child',
          parent_id: parentQdrantId,
          content: child.content,
          chunk_index: child.index,
          metadata: { source_url: url, parent_index: child.parentIndex },
        },
      })
    }

    await batchUpsert(user_id, points, batchSize)
    console.log(`âœ… [Qdrant] ç½‘é¡µ: æˆåŠŸæ’å…¥ ${points.length} ä¸ªå‘é‡ç‚¹`)

    ;(textsToEmbed as any).length = 0
    ;(allEmbeddings as any).length = 0
    ;(points as any).length = 0

    await updateDocStatus(doc_id, 'completed', {
      deep_summary: ktypeResult.finalReport,
      ktype_summary: ktypeSummary,
      ktype_metadata: ktypeMetadata as any,
      chunk_count: parentChunks.length + childChunks.length,
    })

    return { success: true, chunks_count: points.length }
  } catch (error: any) {
    const message = error.message || 'æœªçŸ¥é”™è¯¯'
    await updateDocStatus(doc_id, 'failed', { error_message: message })
    throw error
  }
}
