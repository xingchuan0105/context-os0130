/**
 * ç®€å•çš„ LLM API æµ‹è¯•è„šæœ¬
 * ç”¨äºæµ‹è¯•å„ç§æ¨¡å‹é…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ
 */

import { config } from 'dotenv'
import { resolve } from 'path'
import { createLLMClient, MODEL_CONFIGS } from '../lib/llm-client.js'

// åŠ è½½ç¯å¢ƒå˜é‡
const envPath = resolve(__dirname, '../.env.local')
config({ path: envPath })

async function testLLM() {
  console.log('ğŸ”¬ LLM API æµ‹è¯•')
  console.log('=====================================\n')

  // æµ‹è¯•ç®€å•é—®é¢˜
  const testPrompt = 'ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚'
  const messages = [
    { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚' },
    { role: 'user', content: testPrompt }
  ]

  // æµ‹è¯•é…ç½®
  const tests = [
    {
      name: 'DeepSeek ç›´è¿ (é»˜è®¤é…ç½®)',
      modelKey: 'oneapi', // ä½¿ç”¨ .env ä¸­çš„ ONEAPI é…ç½®
      description: `Base URL: ${process.env.ONEAPI_BASE_URL}\nModel: ${process.env.ONEAPI_MODEL}`
    },
    {
      name: 'DeepSeek ç›´è¿ (hardcoded)',
      modelKey: 'deepseek',
      description: 'Base URL: https://api.deepseek.com/v1\nModel: deepseek-chat'
    },
    {
      name: 'SiliconFlow DeepSeek-V3',
      modelKey: 'siliconflow_deepseek',
      description: 'Base URL: https://api.siliconflow.cn/v1\nModel: deepseek-ai/DeepSeek-V3'
    }
  ]

  for (const test of tests) {
    console.log(`\nğŸ“‹ æµ‹è¯•: ${test.name}`)
    console.log(`   ${test.description}`)
    console.log(`   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`)

    try {
      const startTime = Date.now()
      const client = createLLMClient(test.modelKey as any)
      const config = client.getConfig()

      console.log(`   å®é™…é…ç½®:`)
      console.log(`   - Base URL: ${config.baseURL}`)
      console.log(`   - Model: ${config.model}`)
      console.log(`   - API Key: ${config.apiKey ? config.apiKey.slice(0, 10) + '...' : 'æœªè®¾ç½®'}`)
      console.log(`   - Timeout: ${config.timeout}ms`)

      console.log(`\n   å‘é€è¯·æ±‚: "${testPrompt}"`)

      const { content, duration } = await client.chat(messages, {
        temperature: 0.7,
        maxTokens: 100
      })

      const elapsed = Date.now() - startTime

      console.log(`\n   âœ… æˆåŠŸ!`)
      console.log(`   â±ï¸  è€—æ—¶: ${elapsed}ms (${(elapsed / 1000).toFixed(2)}s)`)
      console.log(`   ğŸ“ å›å¤: ${content.slice(0, 100)}${content.length > 100 ? '...' : ''}`)

    } catch (error: any) {
      console.log(`\n   âŒ å¤±è´¥!`)
      console.log(`   é”™è¯¯: ${error.message}`)

      // æ‰“å°æ›´å¤šé”™è¯¯è¯¦æƒ…
      if (error.cause) {
        console.log(`   Cause: ${error.cause}`)
      }
      if (error.code) {
        console.log(`   Code: ${error.code}`)
      }
      if (error.status) {
        console.log(`   Status: ${error.status}`)
      }
    }
  }

  console.log('\n=====================================')
  console.log('æµ‹è¯•å®Œæˆ!\n')
}

testLLM()
