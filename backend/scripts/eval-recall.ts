import 'dotenv/config'
import { readFileSync, writeFileSync } from 'fs'
import { resolve } from 'path'
import { initializeDatabase, db } from '../lib/db/schema'
import { ragRetrieve, type RAGResult } from '../lib/rag/retrieval'
import { getDocumentsByKbId } from '../lib/db/queries'
import { createLLMClient } from '../lib/llm-client'

type TestCase = { id: string; query: string }

const TEST_SET_PATH = resolve(process.cwd(), 'æµ‹è¯•é›†.json')
const REPORT_PATH = resolve(process.cwd(), 'recall-report.json')
const USER_EMAIL = 'auto-rag@example.com'
const KB_TITLE = 'Auto RAG KB'
const REFLECTION_THRESHOLD = 0.3
const REFLECTION_LOOPS = 2
const TOPK_CHILD = 5

async function selectDocsBySummary(query: string, kbId: string, limit = 3): Promise<string[]> {
  const docs = await getDocumentsByKbId(kbId)
  if (docs.length === 0) return []

  const listText = docs
    .map(
      (d, idx) =>
        `${idx + 1}. doc_id=${d.id}\n   file=${d.file_name}\n   summary=${(d.ktype_summary || d.deep_summary || '').slice(0, 500) || 'n/a'}`,
    )
    .join('\n')

  try {
    const llm = createLLMClient('qwen_flash')
    const { content } = await llm.chat(
      [
        {
          role: 'system',
          content:
            'ä½ æ˜¯æ£€ç´¢è·¯ç”±å™¨ï¼Œç»™å®šæŸ¥è¯¢å’Œæ–‡æ¡£æ‰§è¡Œæ‘˜è¦ï¼Œè¿”å›æœ€ç›¸å…³ doc_id åˆ—è¡¨ï¼ŒJSON è¾“å‡ºï¼š{"doc_ids":["id1","id2"]}ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚',
        },
        {
          role: 'user',
          content: `æŸ¥è¯¢ï¼š${query}\næœ€å¤šè¿”å›${limit} ä¸ªæ–‡æ¡£ã€‚\nå€™é€‰æ–‡æ¡£ï¼š\n${listText}`,
        },
      ],
      { temperature: 0, responseFormat: { type: 'json_object' } },
    )
    const cleaned = content.trim().replace(/^```(?:json)?\s*/i, '').replace(/```$/i, '').trim()
    const parsed = JSON.parse(cleaned)
    const ids = (parsed.doc_ids || parsed.docIds || parsed.documents) as string[] | undefined
    if (Array.isArray(ids)) return ids.filter((id) => typeof id === 'string' && id.trim().length > 0).slice(0, limit)
  } catch (err) {
    console.warn(`[DocRouting] å¤±è´¥ï¼Œfallback: ${err instanceof Error ? err.message : String(err)}`)
  }

  const terms = query.toLowerCase().split(/[^a-z0-9\u4e00-\u9fa5]+/).filter(Boolean)
  return docs
    .map((d) => {
      const summary = (d.ktype_summary || d.deep_summary || '').toLowerCase()
      const score = terms.reduce((acc, t) => (summary.includes(t) ? acc + 1 : acc), 0)
      return { id: d.id, score }
    })
    .sort((a, b) => b.score - a.score)
    .slice(0, limit)
    .map((d) => d.id)
}

function loadTestSet(): TestCase[] {
  const raw = readFileSync(TEST_SET_PATH, 'utf-8')
  const tests: TestCase[] = []
  const lines = raw.split(/\r?\n/)
  for (const line of lines) {
    const m = line.match(/^\s*(\d+)\.\s+(.*\S)\s*$/)
    if (m) {
      const num = parseInt(m[1], 10)
      const id = `q_${String(num).padStart(3, '0')}`
      const query = m[2].trim()
      tests.push({ id, query })
    }
  }
  if (tests.length === 0) {
    throw new Error('æµ‹è¯•é›†è§£æå¤±è´¥ï¼šæœªæ‰¾åˆ°ä»»ä½•â€œç¼–å·. é—®é¢˜â€è¡Œ')
  }
  return tests
}

function ensureUserKb() {
  initializeDatabase()
  const user = db.prepare('SELECT * FROM users WHERE email = ?').get(USER_EMAIL) as any
  if (!user) throw new Error('æ‰¾ä¸åˆ°æµ‹è¯•ç”¨æˆ·ï¼Œè¯·å…ˆè¿è¡Œ ingest æµç¨‹')
  const kb = db
    .prepare('SELECT * FROM knowledge_bases WHERE user_id = ? AND title = ?')
    .get(user.id, KB_TITLE) as any
  if (!kb) throw new Error('æ‰¾ä¸åˆ°æµ‹è¯•çŸ¥è¯†åº“ï¼Œè¯·å…ˆè¿è¡Œ ingest æµç¨‹')
  return { user, kb }
}

function getTopScore(r: RAGResult): number {
  const scores: number[] = []
  if (r.context.document?.score) scores.push(r.context.document.score)
  r.context.parents.forEach((p) => scores.push(p.score))
  r.context.children.forEach((c) => scores.push(c.score))
  return scores.length ? Math.max(...scores) : 0
}

async function rewriteQuery(query: string): Promise<string> {
  const llm = createLLMClient('qwen_flash')
  const prompt = `å½“å‰æ£€ç´¢å¾—åˆ†åä½ï¼Œè¯·æ”¹å†™æŸ¥è¯¢ï¼Œä½¿å…¶æ›´æ˜“å‘½ä¸­èµ„æ–™ï¼š
- è‹¥é—®é¢˜è¿‡äºæŠ½è±¡ï¼Œè¯·å…·ä½“åŒ–ï¼ˆè¡¥å……åœºæ™¯ã€å¯¹è±¡ã€å…³é”®åè¯ï¼‰
- è‹¥é—®é¢˜è¿‡äºç»†ç¢ï¼Œè¯·ç¨ä½œæŠ½è±¡ï¼Œä¿ç•™æ ¸å¿ƒä¸»é¢˜
- ä¿ç•™ä¸­æ–‡ï¼Œè¾“å‡º JSONï¼š{"query": "..."}`
  try {
    const { content } = await llm.chat(
      [
        { role: 'system', content: 'ä½ æ˜¯æ£€ç´¢æŸ¥è¯¢æ”¹å†™å™¨ï¼Œåªèƒ½è¾“å‡º JSONã€‚' },
        { role: 'user', content: `åŸæŸ¥è¯¢ï¼š${query}\n${prompt}` },
      ],
      { temperature: 0, responseFormat: { type: 'json_object' } },
    )
    const cleaned = content.trim().replace(/^```(?:json)?\s*/i, '').replace(/```$/i, '').trim()
    const parsed = JSON.parse(cleaned)
    const q = parsed.query || parsed.new_query || parsed.rewritten_query
    if (typeof q === 'string' && q.trim()) return q.trim()
  } catch (err) {
    console.warn('[Rewrite] å¤±è´¥ï¼Œæ²¿ç”¨åŸæŸ¥è¯¢:', err instanceof Error ? err.message : String(err))
  }
  return query
}

async function generateAnswer(query: string, result: RAGResult): Promise<string> {
  const llm = createLLMClient('deepseek_v32')
  const docText = result.context.document?.payload?.content || ''

  const topChildren = [...result.context.children]
    .sort((a, b) => b.score - a.score)
    .slice(0, TOPK_CHILD)
    .map((c, idx) => `å­å—${idx + 1} (score=${c.score.toFixed(3)}): ${c.payload?.content || ''}`)
  const parents = result.context.parents
    .map((p, idx) => `çˆ¶å—${idx + 1} (score=${p.score.toFixed(3)}): ${p.payload?.content || ''}`)
    .join('\n\n')
  const childText = topChildren.join('\n\n')

  const messages = [
    {
      role: 'system' as const,
      content:
        'ä½ æ˜¯ä¸¥è°¨çš„é—®ç­”åŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼ä¾èµ–æä¾›çš„ä¸Šä¸‹æ–‡ä½œç­”ï¼Œä¸è¦ç¼–é€ ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œè¯·ç›´è¯´â€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚ä¿æŒä¸­æ–‡ä½œç­”ã€‚',
    },
    {
      role: 'user' as const,
      content: `é—®é¢˜ï¼š${query}
æ–‡æ¡£å±‚ KType æŠ¥å‘Šï¼š${docText}
ç›¸å…³çˆ¶å—ï¼š
${parents}

Top${TOPK_CHILD} å­å—ï¼š
${childText}

è¯·åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡ä½œç­”ï¼Œå¼•ç”¨åˆ°çš„è¦ç‚¹ç”¨ç®€çŸ­è¯´æ˜ï¼Œä¸è¦è‡†é€ ã€‚`,
    },
  ]

  try {
    const { content } = await llm.chat(messages, { temperature: 0.2, maxTokens: 800 })
    return content
  } catch (err) {
    console.warn('[ç”Ÿæˆå›ç­”] å¤±è´¥ï¼Œè¿”å›ç©ºç­”æ¡ˆ:', err instanceof Error ? err.message : String(err))
    return ''
  }
}

function gradeWithLLM(query: string, answer: string) {
  const llm = createLLMClient('qwen_flash')
  const prompt = `ä½ æ˜¯ä¸¥æ ¼çš„è¯„åˆ†å‘˜ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†å¯¹ç³»ç»Ÿå›ç­”è¿›è¡Œè¯„åˆ†ï¼ˆ1-5 åˆ†ï¼Œ5 ä¸ºæœ€å¥½ï¼‰ï¼Œå¹¶ä»…è¾“å‡º JSONï¼š
ç»´åº¦ï¼š
1) Comprehensivenessï¼šæ˜¯å¦è¦†ç›–é—®é¢˜å…³é”®ç‚¹/ç»†èŠ‚ï¼Œæ˜¯å¦å……åˆ†åˆ©ç”¨è¯­æ–™çŸ¥è¯†ã€‚
2) Diversityï¼šæ˜¯å¦å¤šè§’åº¦ï¼ˆæŠ€æœ¯/è¡Œä¸º/é•¿è¿œå½±å“ç­‰ï¼‰ç»™å‡ºè§è§£ï¼Œæ˜¯å¦ä½“ç°è·¨å±‚çº§åˆ†æã€‚
3) Empowermentï¼šæ˜¯å¦å¸®åŠ©è¯»è€…ç†è§£å¤æ‚ç³»ç»Ÿé€»è¾‘ï¼Œæ˜¯å¦ç»™å‡ºå¯æ‰§è¡Œçš„æŒ‡å¼•ã€‚
4) Overallï¼šç»¼åˆå‰ä¸‰é¡¹ï¼Œè€ƒè™‘é€»è¾‘è¿è´¯ã€å‡†ç¡®æ€§ã€è¡¨è¿°è´¨é‡ã€‚

è¾“å‡ºæ ¼å¼ï¼š
{
  "Comprehensiveness": { "Score": 0, "Reason": "..." },
  "Diversity": { "Score": 0, "Reason": "..." },
  "Empowerment": { "Score": 0, "Reason": "..." },
  "Overall": { "Score": 0, "Reason": "..." }
}

å¾…è¯„ä¼°é—®é¢˜ï¼š${query}
ç³»ç»Ÿå›ç­”ï¼š${answer.slice(0, 4000)}`

  return llm
    .chat(
      [
        { role: 'system', content: 'ä½ æ˜¯ä¸¥è°¨çš„è¯„åˆ†å‘˜ï¼Œåªèƒ½æŒ‰è¦æ±‚è¾“å‡º JSONã€‚' },
        { role: 'user', content: prompt },
      ],
      { temperature: 0, responseFormat: { type: 'json_object' } },
    )
    .then(({ content }) => {
      const cleaned = content.trim().replace(/^```(?:json)?\s*/i, '').replace(/```$/i, '').trim()
      return JSON.parse(cleaned)
    })
    .catch((err) => {
      console.warn('[LLMè¯„åˆ†] å¤±è´¥ï¼Œè¿”å›ç©ºè¯„åˆ†ï¼š', err instanceof Error ? err.message : String(err))
      return {
        Comprehensiveness: { Score: 0, Reason: 'LLM è¯„åˆ†å¤±è´¥' },
        Diversity: { Score: 0, Reason: 'LLM è¯„åˆ†å¤±è´¥' },
        Empowerment: { Score: 0, Reason: 'LLM è¯„åˆ†å¤±è´¥' },
        Overall: { Score: 0, Reason: 'LLM è¯„åˆ†å¤±è´¥' },
      }
    })
}

async function main() {
  const { user, kb } = ensureUserKb()
  const tests = loadTestSet()
  const metrics: Array<any> = []

  for (const t of tests) {
    let queryCurrent = t.query
    let docIds: string[] = []
    let result: RAGResult | null = null

    for (let i = 0; i <= REFLECTION_LOOPS; i++) {
      docIds = await selectDocsBySummary(queryCurrent, kb.id, 3)
      result = await ragRetrieve(user.id, queryCurrent, {
        kbId: kb.id,
        documentIds: docIds,
        documentLimit: 5,
        parentLimit: 10,
        childLimit: 16,
        scoreThreshold: 0.3,
        rerank: true, // parent/child rerank on
      })
      const topScore = getTopScore(result)
      if (topScore < REFLECTION_THRESHOLD && i < REFLECTION_LOOPS) {
        queryCurrent = await rewriteQuery(queryCurrent)
        continue
      }
      break
    }

    if (!result) {
      console.warn(`[Case ${t.id}] æœªè·å–åˆ°æ£€ç´¢ç»“æœï¼Œè·³è¿‡`)
      continue
    }

    const answer = await generateAnswer(queryCurrent, result)
    const scores = await gradeWithLLM(queryCurrent, answer)

    metrics.push({ id: t.id, query_original: t.query, query_used: queryCurrent, docIds, answer, scores })
    console.log(`ğŸ” ${t.id} done`)
    await new Promise((res) => setTimeout(res, 200))
  }

  const report = { total: metrics.length, cases: metrics }
  writeFileSync(REPORT_PATH, JSON.stringify(report, null, 2), 'utf-8')
  console.log('\nâœ… è¯„åˆ†å®Œæˆï¼ŒæŠ¥å‘Šå·²å†™å…¥', REPORT_PATH)
}

main().catch((err) => {
  console.error(err)
  process.exit(1)
})
