# RAG 召回测试集设计

## 一、测试目标

验证三层 RAG 检索系统的召回质量，确保：
1. 能找到相关文档（文档层召回）
2. 能定位相关章节（父块层召回）
3. 能获取具体细节（子块层召回）
4. 排序合理（相关度高的排在前面）
5. 支持多文档筛选场景

---

## 二、评估指标体系

| 指标 | 定义 | 目标值 |
|------|------|--------|
| **Recall@5** | 前5个结果中相关结果的比例 | ≥80% |
| **Precision@5** | 前5个结果中相关结果的占比 | ≥70% |
| **MRR@5** | 第一个相关结果的倒数排名 | ≥0.7 |
| **NDCG@5** | 考虑排序质量的归一化增益 | ≥0.75 |
| **Layer Coverage** | 三层都有结果的比例 | ≥60% |
| **Latency P95** | 95%请求的响应时间 | ≤3s |

---

## 三、测试用例分类

### 3.1 按查询类型分类

```
查询类型分类树
├── 事实性查询 (Factual)
│   ├── 单一事实: "XX是什么？"
│   ├── 多个事实: "列出XX的三个特点"
│   └── 数值查询: "XX的版本号是多少？"
│
├── 概念性查询 (Conceptual)
│   ├── 定义查询: "什么是XX？"
│   ├── 关系查询: "XX和YY有什么区别？"
│   └── 分类查询: "XX属于哪一类？"
│
├── 程序性查询 (Procedural)
│   ├── 步骤查询: "如何做XX？"
│   ├── 流程查询: "XX的流程是什么？"
│   └── 故障排查: "XX出错怎么办？"
│
├── 比较性查询 (Comparative)
│   ├── 两项比较: "XX和YY哪个更好？"
│   └── 多项比较: "比较XX、YY、ZZ的区别"
│
└── 综合性查询 (Complex)
    ├── 多跳推理: "为什么XX会导致YY？"
    ├── 跨文档: "在文档A中提到的方法在文档B中有应用吗？"
    └── 开放式: "总结一下关于XX的所有信息"
```

### 3.2 按查询难度分类

| 难度 | 特征 | 示例 |
|------|------|------|
| **Easy** | 关键词直接匹配，答案明确 | "如何创建知识库？" |
| **Medium** | 需要语义理解，多步推理 | "为什么向量检索比关键词检索更准确？" |
| **Hard** | 跨章节/跨文档，需要综合 | "比较不同文���中提到的XX方法的优缺点" |
| **Challenge** | 歧义查询，需要消歧 | "部署时需要注意什么？"（多处提到） |

---

## 四、标准测试用例集

### 4.1 事实性查询测试

| ID | 查询 | 预期召回层 | 验证点 |
|----|------|-----------|--------|
| F1 | "ContextOS 支持哪些文件格式？" | document/child | 文档概述+具体格式列表 |
| F2 | "向量维度是多少？" | child | 精确数值 |
| F3 | "BAAI/bge-m3 模型的向量维度是多少？" | child | 技术参数 |
| F4 | "Qdrant 默认端口是什么？" | child | 配置参数 |

### 4.2 概念性查询测试

| ID | 查询 | 预期召回层 | 验证点 |
|----|------|-----------|--------|
| C1 | "什么是 KTYPE 分析？" | document/parent | 定义+解释 |
| C2 | "父子分块策略是什么？" | document/parent | 概念解释 |
| C3 | "RAG 检索的三层架构是什么？" | document/parent | 架构说明 |
| C4 | "向量数据库和关系数据库有什么区别？" | parent | 对比说明 |

### 4.3 程序性查询测试

| ID | 查询 | 预期召回层 | 验证点 |
|----|------|-----------|--------|
| P1 | "如何上传文档？" | child | 操作步骤 |
| P2 | "配置 Qdrant 的步骤是什么？" | parent/child | 配置流程 |
| P3 | "如何创建知识库？" | child | 创建步骤 |
| P4 | "文档上传失败怎么办？" | child | 故障排查 |

### 4.4 比较性查询测试

| ID | 查询 | 预期召回层 | 验证点 |
|----|------|-----------|--------|
| CP1 | "父子分块和普通分块有什么区别？" | parent | 对比说明 |
| CP2 | "Cosine 距离和 Euclidean 距离哪个更适合向量检索？" | child | 适用场景 |
| CP3 | "COS 和本地存储有什么区别？" | parent | 存储方案对比 |

### 4.5 综合性查询测试

| ID | 查询 | 预期召回层 | 验证点 |
|----|------|-----------|--------|
| X1 | "为什么要用三层检索而不是直接检索子块？" | document/parent | 设计意图 |
| X2 | "完整的文档处理流程是什么？" | document/parent | 端到端流程 |
| X3 | "KTYPE 分析结果如何影响检索质量？" | document/parent/child | 因果关系 |

### 4.6 边界情况测试

| ID | 查询 | 类型 | 预期行为 |
|----|------|------|----------|
| B1 | "量子力学的基本原理" | 无关 | 返回空或低分 |
| B2 | "部署" | 歧义 | 返回多个相关章节 |
| B3 | "a" | 过短 | 返回空或提示 |
| B4 | "怎么在系统里找到我去年上传的PDF并重新索引？" | 复杂 | 多文档/多操作 |

### 4.7 多文档筛选测试

| ID | 选中文档 | 查询 | 验证点 |
|----|----------|------|--------|
| M1 | [技术文档] | "如何配置？" | 只在技术文档中搜索 |
| M2 | [用户手册, 技术文档] | "支持的文件格式" | 合并两个文档的结果 |
| M3 | [] (未选) | "如何使用？" | 搜索整个知识库 |

---

## 五、评估标准

### 5.1 相关性评级标准

| 评级 | 描述 | 示例 |
|------|------|------|
| **3 - 高度相关** | 直接回答问题，信息完整 | 查询"如何上传文档？"，召回包含完整上传步骤的内容 |
| **2 - 部分相关** | 有相关信息但不完整 | 查询"如何配置Qdrant？"，召回只提到Qdrant名称的内容 |
| **1 - 弱相关** | 主题相关但无具体答案 | 查询"端口是多少？"，召回介绍Qdrant的内容 |
| **0 - 不相关** | 与查询无关 | 查询"上传文档"，召回关于向量维度的内容 |

### 5.2 分层召回质量标准

| 层级 | 评估标准 |
|------|----------|
| **Document 层** | KTYPE摘要应包含查询主题的概述，主导类型匹配 |
| **Parent 层** | 父块应包含相关章节的完整上下文，相关度 ≥0.5 |
| **Child 层** | 子块应包含具体答案，相关度 ≥0.6 |

### 5.3 综合评分计算

```
综合得分 = (Recall@5 × 0.3) + (Precision@5 × 0.3) + (MRR@5 × 0.2) + (NDCG@5 × 0.2)

等级划分：
- 优秀 (Excellent): ≥0.85
- 良好 (Good): 0.70 - 0.84
- 及格 (Fair): 0.55 - 0.69
- 不及格 (Poor): <0.55
```

---

## 六、测试执行模板

### 6.1 单条测试记录表

| 字段 | 说明 |
|------|------|
| 测试ID | 如 "F1", "C1" |
| 查询内容 | 用户输入的查询 |
| 参数设置 | scoreThreshold, limits 等 |
| 召回结果 | Document/Parent/Child 各层的召回数 |
| 相关性评分 | 每个结果的相关性评级 (0-3) |
| 指标计算 | Recall@5, Precision@5, MRR@5 |
| 问题记录 | 发现的问题 |

### 6.2 测试结果汇总表

| 类别 | 用例数 | 通过数 | 平均Recall | 平均Precision | 平均MRR |
|------|--------|--------|-----------|--------------|---------|
| 事实性 | | | | | |
| 概念性 | | | | | |
| 程序性 | | | | | |
| 比较性 | | | | | |
| 综合性 | | | | | |
| 边界情况 | | | | | |
| **总计** | | | | | |

---

## 七、测试工具建议

### 7.1 自动化测试脚本结构

```typescript
// 测试用例定义
interface TestCase {
  id: string
  query: string
  category: string
  difficulty: 'easy' | 'medium' | 'hard' | 'challenge'
  expected?: {
    minResults: number
    expectedDoc?: string  // 预期召回的文档
    relevantKeywords: string[]  // 相关关键词
  }
}

// 测试执行
interface TestResult {
  testCaseId: string
  query: string
  results: {
    document: SearchResult | null
    parents: SearchResult[]
    children: SearchResult[]
  }
  metrics: {
    recall: number
    precision: number
    mrr: number
    ndcg: number
  }
  evaluation: {
    relevant: number[]
    scores: number[]
  }
}
```

### 7.2 可视化建议

- **召回覆盖率图**：饼图展示各层级的召回比例
- **相关度分布图**：柱状图展示各相关度等级的数量
- **查询类型热力图**：不同类型查询的性能对比
- **延迟分布图**：P50/P95/P99 响应时间

---

## 八、测试环境准备

1. **测试数据**：准备至少 10 个不同领域的文档
2. **用户模拟**：准备不同角色（开发者、用户、管理员）的查询
3. **A/B 对比**：准备传统检索 vs 三层检索的对比
4. **压力测试**：并发查询测试召回稳定性

---

## 九、讨论议题

### 9.1 召回策略讨论

- 当前 scoreThreshold=0.3 是否合适？
- parentLimit=2, childLimit=5 是否需要动态调整？
- 是否需要引入重排序（Reranker）？

### 9.2 三层架构讨论

- Document 层的 KTYPE 摘要是否足够精确？
- Parent 层的章节划分粒度是否合理？
- Child 层的块大小（256字符）是否最优？

### 9.3 扩展性讨论

- 如何处理跨文档的复杂查询？
- 是否需要支持多轮对话的上下文累积？
- 如何处理用户选中多个文档的场景？
