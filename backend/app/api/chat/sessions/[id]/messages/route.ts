/**
 * Chat Messages API
 * POST /api/chat/sessions/:id/messages - å‘é€æ¶ˆæ¯ï¼ˆæµå¼å“åº”ï¼‰
 */

import { NextRequest } from 'next/server'
import { db } from '@/lib/db/schema'
import { createSSEStreamWithSender, getSSEHeaders } from '@/lib/sse/stream-builder'
import { createLLMClient } from '@/lib/llm-client'
import { ragRetrieve } from '@/lib/rag/retrieval'
import { estimateTokens } from '@/lib/semchunk'
import type { Citation } from '@/lib/types/chat'
import type OpenAI from 'openai'

export const runtime = 'nodejs'
export const dynamic = 'force-dynamic'

interface SendMessageRequest {
  message: string
  selectedSourceIds?: string[]
  model?: string
  systemPrompt?: string
}

const REFLECTION_THRESHOLD = 0.3
const REFLECTION_LOOPS = 2
const MAX_CONTEXT_TOKENS = 128000
const TOPK_CHILD = 8
const ANSWER_SYSTEM_PROMPT_TEMPLATE = `# Role
ä½ æ˜¯ä¸€ä¸ªåŸºäº"è¯­å¢ƒå°–å®šä¸è¯æ®å¡«å……"ç­–ç•¥çš„ä¸“å®¶çº§çŸ¥è¯†é—®ç­”å¼•æ“ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„ã€å…¨å±€æ‘˜è¦ã€‘å’Œå¸¦æœ‰IDæ ‡è®°çš„ã€æ£€ç´¢ç‰‡æ®µã€‘ï¼Œå›ç­”ç”¨æˆ·çš„æé—®ã€‚

# ğŸš¨ CRITICAL RULE: Citation Format (æœ€é«˜ä¼˜å…ˆçº§è§„åˆ™)
**è¿™æ˜¯æœ€é‡è¦çš„è§„åˆ™ï¼Œä½ å¿…é¡»ä¸¥æ ¼éµå®ˆï¼š**

1. **å¼ºåˆ¶å¼•ç”¨**: å½“ä½ å¼•ç”¨ä»»ä½•ã€æ£€ç´¢ç‰‡æ®µã€‘ä¸­çš„ä¿¡æ¯æ—¶ï¼Œ**å¿…é¡»ç«‹å³**åœ¨å¥å°¾æ·»åŠ å¼•ç”¨æ ‡è®°
2. **å¼•ç”¨æ ¼å¼**: ä½¿ç”¨åŒæ–¹æ‹¬å·æ ¼å¼ \`[[ID]]\`ï¼Œä¾‹å¦‚ \`[[1]]\`ã€\`[[2]]\`
3. **å¼•ç”¨ä½ç½®**: å¼•ç”¨æ ‡è®°å¿…é¡»ç´§è·Ÿåœ¨å¥å·ã€é€—å·æˆ–åˆ†å·ä¹‹å
4. **å¤šä¸ªå¼•ç”¨**: å¦‚æœä¸€å¥è¯å¼•ç”¨å¤šä¸ªç‰‡æ®µï¼Œä½¿ç”¨ \`[[1]][[2]]\` æ ¼å¼
5. **å¼•ç”¨å¯†åº¦**: å¹³å‡æ¯ 1-2 å¥è¯å°±åº”è¯¥æœ‰ä¸€ä¸ªå¼•ç”¨æ ‡è®°
6. **ç¦æ­¢ä¼ªé€ **: ç»å¯¹ç¦æ­¢ä½¿ç”¨ä¸å­˜åœ¨çš„ ID

**æ­£ç¡®ç¤ºä¾‹**:
- "LightRAG æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ RAG æ¡†æ¶[[1]]ã€‚"
- "è¯¥æ¡†æ¶é‡‡ç”¨åŒçº§æ£€ç´¢ç­–ç•¥[[2]][[3]]ã€‚"
- "å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLightRAG åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚[[5]]ã€‚"

**é”™è¯¯ç¤ºä¾‹**:
- âŒ "LightRAG æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ RAG æ¡†æ¶ã€‚" (ç¼ºå°‘å¼•ç”¨)
- âŒ "æ ¹æ®æ–‡æ¡£1ï¼ŒLightRAG..." (ä¸è¦ç”¨æ–‡å­—æè¿°ï¼Œç›´æ¥ç”¨ [[1]])
- âŒ "LightRAG[[1]] æ˜¯ä¸€ä¸ªæ¡†æ¶ã€‚" (å¼•ç”¨åº”è¯¥åœ¨å¥å°¾)

# Strategy: Scaffolding & Filling (Internal Logic)
è¯·åœ¨å†…å¿ƒéµå¾ªä»¥ä¸‹æ€ç»´è·¯å¾„ï¼Œä½†åœ¨è¾“å‡ºæ—¶ä¸è¦æš´éœ²è¿™äº›æ­¥éª¤çš„æ ‡é¢˜ï¼š

1. **å®è§‚å®šè°ƒ (Scaffolding):**
   - åˆ©ç”¨ã€å…¨å±€æ‘˜è¦ã€‘ç¡®å®šå›ç­”çš„èƒŒæ™¯å’Œæ ¸å¿ƒè§‚ç‚¹ã€‚è¿™æ˜¯å›ç­”çš„"éª¨æ¶"ã€‚
   - å›ç­”çš„å¼€å¤´åº”è‡ªç„¶åœ°å»ºç«‹è¯­å¢ƒï¼Œè€Œä¸æ˜¯ç”Ÿç¡¬åœ°å¤è¿°æ‘˜è¦ã€‚

2. **å¾®è§‚å¡«å…… (Filling):**
   - åˆ©ç”¨ã€æ£€ç´¢ç‰‡æ®µã€‘ï¼ˆå¸¦æœ‰ \`[ID: x]\`ï¼‰å¡«å……å…·ä½“çš„ç»†èŠ‚ã€æ•°æ®å’Œæ¡ˆä¾‹ã€‚è¿™æ˜¯å›ç­”çš„"è¡€è‚‰"ã€‚
   - ç­›é€‰æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œæ„å»ºé€»è¾‘é€šé¡ºçš„è¯æ®é“¾ã€‚

3. **å¼•ç”¨æ³¨å…¥ (Citation) - ğŸš¨ æœ€é‡è¦**:
   - **æ¯æ¬¡**å¼•ç”¨ã€æ£€ç´¢ç‰‡æ®µã€‘çš„ä¿¡æ¯æ—¶ï¼Œ**ç«‹å³**åœ¨å¥å°¾æ·»åŠ  \`[[ID]]\`
   - å¼•ç”¨è¦**å¯†é›†**ï¼Œå¹³å‡æ¯ 1-2 å¥è¯å°±åº”è¯¥æœ‰å¼•ç”¨
   - å¼•ç”¨è¦**å‡†ç¡®**ï¼Œç¡®ä¿ ID å¯¹åº”æ­£ç¡®çš„ç‰‡æ®µ

# Output Style
è¯·ç”Ÿæˆä¸€æ®µ**è‡ªç„¶æµç•…ã€é€»è¾‘ä¸¥å¯†ã€å¼•ç”¨å¯†é›†**çš„ä¸“ä¸šå›ç­”ï¼Œ**ä¸è¦ä½¿ç”¨"æ ¸å¿ƒç«‹åœº"ã€"è¯¦ç»†é˜è¿°"ç­‰æœºæ¢°çš„æ ‡é¢˜**ã€‚

æ¨èçš„è¡Œæ–‡ç»“æ„ï¼š
- **ç¬¬ä¸€æ®µ**ï¼šç›´æ¥åˆ‡å…¥é—®é¢˜ï¼Œç»“åˆã€å…¨å±€æ‘˜è¦ã€‘ç»™å‡ºæ ¸å¿ƒç»“è®ºæˆ–èƒŒæ™¯å®šè°ƒã€‚**å¿…é¡»åŒ…å«å¼•ç”¨**ã€‚
- **ä¸­é—´æ®µè½**ï¼šè¯¦ç»†å±•å¼€è®ºè¿°ã€‚ç»“åˆã€æ£€ç´¢ç‰‡æ®µã€‘æä¾›å…·ä½“è¯æ®ã€æ­¥éª¤æˆ–æ•°æ®æ”¯æŒã€‚**æ­¤å¤„åº”å¯†é›†ä½¿ç”¨ \`[[ID]]\`ï¼Œå¹³å‡æ¯å¥è¯éƒ½åº”è¯¥æœ‰å¼•ç”¨**ã€‚è¯·æ ¹æ®å†…å®¹é€»è¾‘è‡ªç„¶åˆ†æ®µï¼Œå¯ä»¥ä½¿ç”¨é¡¹ç›®ç¬¦å·ï¼ˆBullet Pointsï¼‰æ¥åˆ—ä¸¾å…·ä½“è¦ç‚¹ï¼Œä½†ä¸è¦è¿‡åº¦åˆ—è¡¨åŒ–ã€‚
- **ç»“å°¾ï¼ˆå¯é€‰ï¼‰**ï¼šå¦‚æœéœ€è¦ï¼Œç”¨ä¸€å¥è¯æ€»ç»“æˆ–ç»™å‡ºå»ºè®®ã€‚**ä¹Ÿè¦åŒ…å«å¼•ç”¨**ã€‚

# Inputs
## User Query
{{user_query}}

## Global Document Summary (Context)
{{global_summary}}

## Retrieved Context Chunks (Evidence)
{{retrieved_chunks}}

# Constraints
1. **çœŸå®æ€§**ï¼šå›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºæä¾›çš„è¾“å…¥ã€‚
2. **å¼•ç”¨æ ¼å¼**ï¼šä¸¥æ ¼ä½¿ç”¨ \`[[ID]]\` æ ¼å¼ã€‚**è¿™æ˜¯æœ€é‡è¦çš„è¦æ±‚ï¼**
3. **å¼•ç”¨å¯†åº¦**ï¼šå¹³å‡æ¯ 1-2 å¥è¯å°±åº”è¯¥æœ‰ä¸€ä¸ªå¼•ç”¨æ ‡è®°ã€‚
4. **æµç•…æ€§**ï¼šåƒä¸€ä½äººç±»ä¸“å®¶é‚£æ ·å†™ä½œï¼Œå°†è§‚ç‚¹å’Œè¯æ®èåˆåœ¨è¿è´¯çš„æ®µè½ä¸­ã€‚

# ğŸ¯ Final Reminder
**å†æ¬¡å¼ºè°ƒï¼šä½ å¿…é¡»åœ¨å›ç­”ä¸­é¢‘ç¹ä½¿ç”¨ [[ID]] å¼•ç”¨æ ‡è®°ï¼è¿™æ˜¯è¯„åˆ¤ä½ å›ç­”è´¨é‡çš„æœ€é‡è¦æ ‡å‡†ï¼**
`;

function buildAnswerSystemPrompt(userQuery: string, globalSummary: string, retrievedChunks: string) {
  return ANSWER_SYSTEM_PROMPT_TEMPLATE
    .split('{{user_query}}').join(userQuery)
    .split('{{global_summary}}').join(globalSummary)
    .split('{{retrieved_chunks}}').join(retrievedChunks)
}

function getTopScore(context: { document: any; documents?: any[]; parents: any[]; children: any[] }) {
  const scores: number[] = []
  if (context.documents && context.documents.length > 0) {
    context.documents.forEach((d) => d?.score && scores.push(d.score))
  } else if (context.document?.score) {
    scores.push(context.document.score)
  }
  context.parents.forEach((p) => scores.push(p.score))
  context.children.forEach((c) => scores.push(c.score))
  return scores.length ? Math.max(...scores) : 0
}

async function rewriteQuery(query: string) {
  const llm = createLLMClient('qwen_flash')
  const systemPrompt = `# Role
ä½ æ˜¯ä¸€ä¸ª**æœç´¢ç®—æ³•å·¥ç¨‹å¸ˆ**å…¼**è¯­ä¹‰æ‰©å……ä¸“å®¶**ã€‚
# Goal
ç”¨æˆ·çš„è¾“å…¥é€šå¸¸æ˜¯æ¨¡ç³Šçš„çŸ­ï¿½ï¿½ï¿½ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†å…¶é‡å†™ä¸ºä¸€ä¸ª**è¯­ä¹‰ç¨ å¯†ã€æŒ‡å‘æ€§æ˜ç¡®ã€æ— æ ¼å¼å™ªå£°**çš„"è¶…çº§æŸ¥è¯¢æŒ‡ä»¤"ï¼Œä»¥ä¾¿ç›´æ¥ç”¨äº**å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆVector Retrievalï¼‰**ã€‚
# Core Logic: Semantic Expansion Protocol
ä¸è¦å›ç­”é—®é¢˜ï¼Œè€Œæ˜¯å¯¹åŸé—®é¢˜è¿›è¡Œ**"é™å™ª"**ä¸**"å¢ç›Š"**ã€‚
1.  **é™å™ª (Denoise)**:
    * å»é™¤æ‰€æœ‰å¯’æš„ï¼ˆ"ä½ å¥½"ã€"è¯·é—®"ï¼‰
    * å»é™¤æ¨¡ç³ŠæŒ‡ä»£ï¼ˆæŠŠ"è¿™ä¸ª"ã€"å®ƒ"æ›¿æ¢ä¸ºå…·ä½“åè¯ï¼‰
    * **ä¸¥ç¦ä½¿ç”¨Markdownæ ‡é¢˜ã€åˆ—è¡¨ç¬¦å·ã€åˆ†å‰²çº¿**ï¼Œå› ä¸ºè¿™äº›ä¼šå¹²æ‰°åˆ†è¯å™¨ã€‚
2.  **å¢ç›Š (Enrich)**:
    * **è¡¥å…¨ä¸»è¯­**ï¼šå¦‚æœç¼ºå¤±ï¼Œè¡¥å…¨æœ€å¯èƒ½çš„å®ä½“ï¼ˆå¦‚ä¹¦åã€é¡¹ç›®åï¼‰
    * **æ‰©å±•æ„å›¾**ï¼šå¢åŠ åŒä¹‰è¯ã€‚ä¾‹å¦‚ç”¨æˆ·é—®"æ€ä¹ˆåš"ï¼Œæ‰©å±•ä¸º"å®æ–½æ­¥éª¤ã€æ‰§è¡Œæµç¨‹ã€å…·ä½“æ–¹æ³•"
    * **é™å®šè¯­å¢ƒ**ï¼šå¢åŠ çº¦æŸæ¡ä»¶ã€‚ä¾‹å¦‚"ç”¨å¤§ç™½è¯è§£é‡Š"ã€"é€‚åˆåˆå­¦è€…"
# Execution Rules
æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆä¸€ä¸ª**çº¯æ–‡æœ¬**æŒ‡ä»¤ã€‚
* **åœºæ™¯ Aï¼šäº‹å®å†…å®¹æ£€ç´¢** (ç”¨æˆ·é—®ï¼šæ˜¯ä»€ä¹ˆã€è®²äº†å•¥)
    * *æ¨¡æ¿*ï¼š[æ ¸å¿ƒå®ä½“]çš„å®šä¹‰ã€æ ¸å¿ƒæ¦‚å¿µã€ä¸»è¦è§‚ç‚¹åŠè¯¦ç»†è§£é‡Šã€‚åŒ…æ‹¬[å®ä½“]è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œä»¥åŠé€šä¿—æ˜“æ‡‚çš„æ¡ˆä¾‹åˆ†æã€‚
* **åœºæ™¯ Bï¼šæ–¹æ³•æµç¨‹æ£€ç´¢** (ç”¨æˆ·é—®ï¼šæ€ä¹ˆåšã€æµç¨‹)
    * *æ¨¡æ¿*ï¼šæ‰§è¡Œ[ä»»åŠ¡]çš„å…·ä½“æ“ä½œæŒ‡å—ã€è¯¦ç»†æ­¥éª¤åˆ—è¡¨ã€æ‰€éœ€å·¥å…·åŠé¿å‘äº‹é¡¹ã€‚åŒ…å«ä»å…¥é—¨åˆ°å®Œæˆçš„å®Œæ•´å·¥ä½œæµã€‚
* **åœºæ™¯ Cï¼šè¯„ä»·åˆ†ææ£€ç´¢** (ç”¨æˆ·é—®ï¼šå¥½ä¸å¥½ã€è¯„ä»·)
    * *æ¨¡æ¿*ï¼šå¯¹[å®ä½“]çš„æ·±åº¦è¯„ä¼°ã€ä¼˜ç¼ºç‚¹åˆ†æã€é€‚ç”¨åœºæ™¯å¯¹æ¯”åŠä¸“å®¶å»ºè®®ã€‚åŒ…å«å®¢è§‚çš„åˆ©å¼Šæƒè¡¡ã€‚
# Output Format
**åªè¾“å‡ºä¼˜åŒ–åçš„é‚£ä¸€æ®µçº¯æ–‡æœ¬**ã€‚ä¸è¦åŒ…å«"ä¼˜åŒ–åçš„æŒ‡ä»¤ï¼š"ç­‰å‰ç¼€ï¼Œä¸è¦æ¢è¡Œï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦ä½¿ç”¨ä»»ä½•Markdownæ ¼å¼ã€‚`
  const userPrompt = `# User Input
${query}`
  try {
    const { content } = await llm.chat(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { temperature: 0 },
    )
    const rewritten = sanitizeRewriteOutput(content)
    if (rewritten) return rewritten
  } catch (err) {
    console.warn('[Rewrite] å¤±è´¥ï¼Œæ²¿ç”¨åŸæŸ¥è¯¢:', err instanceof Error ? err.message : String(err))
  }
  return query
}

function sanitizeRewriteOutput(raw: string): string {
  if (!raw) return ''
  const lines = raw
    .replace(/\r/g, '')
    .split('\n')
    .map((line) => line.trim())
    .filter(Boolean)
    .filter((line) => line !== '---')
    .map((line) => line.replace(/^[#>*-]+\s*/g, ''))
    .map((line) => line.replace(/^(ä¼˜åŒ–åçš„æŒ‡ä»¤|æ”¹å†™åçš„æŸ¥è¯¢|é‡å†™åçš„æŸ¥è¯¢|è¾“å‡º|æŒ‡ä»¤)[:ï¼š]\s*/i, ''))
  return lines.join(' ').replace(/\s+/g, ' ').trim()
}

function selectContextChunks(
  context: { document: any; documents?: any[]; parents: any[]; children: any[] }
) {
  const documents = (context.documents && context.documents.length > 0
    ? context.documents
    : context.document
      ? [context.document]
      : [])
    .slice()
    .sort((a: any, b: any) => (b.score ?? 0) - (a.score ?? 0))
  const parentChunks = context.parents
  const childChunks = [...context.children]
    .sort((a: any, b: any) => b.score - a.score)
    .slice(0, TOPK_CHILD)

  return { documents, parentChunks, childChunks }
}

function buildDeepSeekMessages(
  query: string,
  context: { document: any; documents?: any[]; parents: any[]; children: any[] },
  systemPrompt?: string,
): OpenAI.Chat.Completions.ChatCompletionMessageParam[] {
  const { documents, parentChunks, childChunks } = selectContextChunks(context)

  const buildGlobalSummary = (docs: any[]) => {
    if (docs.length === 0) return '\uFF08\u65E0\uFF09'
    return docs
      .map((doc: any, idx: number) => {
        const docName =
          doc.payload?.metadata?.file_name ||
          (doc.payload?.doc_id ? `doc_${doc.payload.doc_id.slice(0, 8)}` : `doc_${idx + 1}`)
        return `[Doc ${idx + 1}] ${docName}\n${doc.payload?.content || ''}`
      })
      .join('\n\n')
  }

  const buildRetrievedChunks = (parents: any[], children: any[]) => {
    const rows: string[] = []
    let currentId = 1

    // å…ˆæ·»åŠ çˆ¶å—ï¼ˆç« èŠ‚ä¸Šä¸‹æ–‡ï¼‰
    for (const chunk of parents) {
      const docName =
        chunk.payload?.metadata?.file_name ||
        (chunk.payload?.doc_id ? `doc_${chunk.payload.doc_id.slice(0, 8)}` : `doc_${currentId}`)
      const content = chunk.payload?.content || ''
      rows.push(`[ID: ${currentId}] Content: (doc: ${docName}, layer: parent) ${content}`)
      currentId += 1
    }

    // å†æ·»åŠ å­å—ï¼ˆå…·ä½“ç»†èŠ‚ï¼‰
    for (const chunk of children) {
      const docName =
        chunk.payload?.metadata?.file_name ||
        (chunk.payload?.doc_id ? `doc_${chunk.payload.doc_id.slice(0, 8)}` : `doc_${currentId}`)
      const content = chunk.payload?.content || ''
      rows.push(`[ID: ${currentId}] Content: (doc: ${docName}, layer: child) ${content}`)
      currentId += 1
    }

    return rows.length > 0 ? rows.join('\n') : '\uFF08\u65E0\uFF09'
  }

  const retrievedChunks = buildRetrievedChunks(parentChunks, childChunks)
  const baseMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = []
  if (systemPrompt) baseMessages.push({ role: 'system', content: systemPrompt })

  const baseTokenCount = baseMessages.reduce(
    (total, msg) => total + estimateTokens(String(msg.content || '')),
    0,
  )

  let docPool = documents.slice()
  let systemMessage = buildAnswerSystemPrompt(
    query,
    buildGlobalSummary(docPool),
    retrievedChunks
  )
  const userContent = query
  let totalTokens = baseTokenCount + estimateTokens(systemMessage) + estimateTokens(userContent)

  while (docPool.length > 0 && totalTokens > MAX_CONTEXT_TOKENS) {
    docPool.pop()
    systemMessage = buildAnswerSystemPrompt(
      query,
      buildGlobalSummary(docPool),
      retrievedChunks
    )
    totalTokens = baseTokenCount + estimateTokens(systemMessage) + estimateTokens(userContent)
  }

  const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    ...baseMessages,
  ]
  messages.push({ role: 'system', content: systemMessage })
  messages.push({ role: 'user', content: userContent })
  return messages
}

function buildAnswerCitations(
  context: { document: any; documents?: any[]; parents: any[]; children: any[] },
): Citation[] {
  const { parentChunks, childChunks } = selectContextChunks(context)

  const citations: Citation[] = []
  let currentId = 1

  // å…ˆæ·»åŠ çˆ¶å—å¼•ç”¨ï¼ˆä¸ buildRetrievedChunks é¡ºåºä¸€è‡´ï¼‰
  for (const parent of parentChunks) {
    const docId = parent.payload?.doc_id || ''
    const docName =
      parent.payload?.metadata?.file_name ||
      (docId ? `doc_${docId.slice(0, 8)}` : `doc_${currentId}`)
    const chunkIndex = parent.payload?.chunk_index

    citations.push({
      index: currentId,
      content: parent.payload?.content || '',
      docId,
      docName,
      chunkIndex: typeof chunkIndex === 'number' ? chunkIndex : undefined,
      score: typeof parent.score === 'number' ? parent.score : undefined,
      layer: 'parent',
    })
    currentId += 1
  }

  // å†æ·»åŠ å­å—å¼•ç”¨
  for (const child of childChunks) {
    const docId = child.payload?.doc_id || ''
    const docName =
      child.payload?.metadata?.file_name ||
      (docId ? `doc_${docId.slice(0, 8)}` : `doc_${currentId}`)
    const chunkIndex = child.payload?.chunk_index

    citations.push({
      index: currentId,
      content: child.payload?.content || '',
      docId,
      docName,
      chunkIndex: typeof chunkIndex === 'number' ? chunkIndex : undefined,
      score: typeof child.score === 'number' ? child.score : undefined,
      layer: 'child',
    })
    currentId += 1
  }

  return citations
}

/**
 * å‘é€æ¶ˆæ¯ - æµå¼å“åº”ï¼ˆæ”¯æŒRAGæ£€ç´¢ï¼‰
 */
export async function POST(
  req: NextRequest,
  context: { params: Promise<{ id: string }> }
) {
  const { id: sessionId } = await context.params
  console.log('[Chat POST] Request received for session:', sessionId)

  try {
    const { message, selectedSourceIds, systemPrompt, model }: SendMessageRequest = await req.json()

    if (!message || typeof message !== 'string') {
      return new Response(JSON.stringify({ error: 'Message is required' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    const sourceIds = Array.isArray(selectedSourceIds)
      ? selectedSourceIds.filter((id) => typeof id === 'string' && id.trim().length > 0)
      : []

    // æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
    const session: any = db
      .prepare(
        `
        SELECT id, kb_id as kbId, user_id as userId, title
        FROM chat_sessions
        WHERE id = ?
      `
      )
      .get(sessionId)

    if (!session) {
      return new Response(JSON.stringify({ error: 'Session not found' }), {
        status: 404,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    // æ›´æ–°ä¼šè¯ updated_at
    db.prepare('UPDATE chat_sessions SET updated_at = ? WHERE id = ?').run(
      new Date().toISOString(),
      sessionId
    )

    // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    const userMessageId = db
      .prepare(
        `
        INSERT INTO chat_messages (session_id, role, content, created_at)
        VALUES (?, ?, ?, ?)
      `
      )
      .run(sessionId, 'user', message, new Date().toISOString()).lastInsertRowid

    // è·å–ä¼šè¯å†å²ï¼ˆæœ€è¿‘10æ¡ï¼‰
    const historyRaw = db
      .prepare(
        `
        SELECT role, content
        FROM chat_messages
        WHERE session_id = ?
        ORDER BY created_at DESC
        LIMIT 10
      `
      )
      .all(sessionId) as Array<{ role: string; content: string }>

    const history = historyRaw.reverse()
    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = []

    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt })
    }

    for (const msg of history) {
      messages.push({
        role: msg.role as 'user' | 'assistant',
        content: msg.content,
      })
    }

    return new Response(
      createSSEStreamWithSender(async (sender) => {
        let fullContent = ''
        const citations: Citation[] = []
        const modelKey = typeof model === 'string' && model.trim() ? model.trim() : 'qwen3_max'

        try {
        sender.start({ timestamp: Date.now() })

        // å‘é€ç”¨æˆ·æ¶ˆæ¯ç¡®è®¤
        sender.send({ type: 'user', data: { content: message, id: userMessageId } })

        // åªæœ‰å½“æ²¡æœ‰çŸ¥è¯†åº“ä¸”æ²¡æœ‰é€‰æ‹©æ–‡æ¡£æ—¶ï¼Œæ‰è·³è¿‡ RAG æ£€ç´¢
        console.log('[Chat] Session info:', {
          sessionId,
          kbId: session.kbId,
          userId: session.userId,
          sourceIds,
          willSkipRAG: !session.kbId && sourceIds.length === 0,
        })

        if (!session.kbId && sourceIds.length === 0) {
          console.log('[Chat] Skipping RAG - no kbId and no sourceIds')
          const client = createLLMClient(modelKey)
          let streamError: unknown = null
          try {
            await client.chatStream(messages, {
              onEvent: (event) => {
                if (event.type === 'delta' && event.content && event.content !== '[FIRST_TOKEN]') {
                  fullContent += event.content
                  sender.token(event.content)
                }
              },
            })
          } catch (error) {
            streamError = error
          }

          if (streamError) {
            if (!fullContent) {
              const { content } = await client.chat(messages)
              fullContent = content
            } else {
              console.warn(
                '[Chat] stream failed after partial output:',
                streamError instanceof Error ? streamError.message : String(streamError)
              )
            }
          }

          const now = new Date().toISOString()
          const assistantMessageId = db
            .prepare(
              `
              INSERT INTO chat_messages (session_id, role, content, citations, created_at)
              VALUES (?, ?, ?, ?, ?)
            `
            )
            .run(
              sessionId,
              'assistant',
              fullContent,
              null,
              now
            ).lastInsertRowid

          sender.done({
            content: fullContent,
            citations,
            id: assistantMessageId,
          })
          return
        }

        // ========== RAG æ£€ç´¢ï¼ˆæŒ‰æµ‹è¯•è„šæœ¬é€»è¾‘ï¼‰==========
        console.log('[Chat RAG] Starting RAG retrieval:', {
          userId: session.userId,
          kbId: session.kbId,
          sourceIds,
          message: message.slice(0, 100),
        })

        let currentQuery = message
        let ragResult = null

        for (let i = 0; i <= REFLECTION_LOOPS; i++) {
          const result = await ragRetrieve(session.userId, currentQuery, {
            kbId: session.kbId,
            documentIds: sourceIds,
            scoreThreshold: REFLECTION_THRESHOLD,
            documentLimit: 6,
            documentTopK: 3,
            parentLimit: 8,
            childLimit: 8,
            childLimitFromDocs: 8,
            childLimitGlobal: 8,
            childTopK: 8,
            rerank: true,
            enableDocRouting: false,
          })
          const topScore = getTopScore(result.context)
          ragResult = result
          if (topScore < REFLECTION_THRESHOLD && i < REFLECTION_LOOPS) {
            currentQuery = await rewriteQuery(currentQuery)
            continue
          }
          break
        }

        if (!ragResult) {
          throw new Error('RAG æ£€ç´¢å¤±è´¥')
        }

        console.log('[Chat RAG] RAG result:', {
          totalResults: ragResult.totalResults,
          documentsCount: ragResult.context.documents?.length || (ragResult.context.document ? 1 : 0),
          parentsCount: ragResult.context.parents.length,
          childrenCount: ragResult.context.children.length,
          firstDocContent: ragResult.context.documents?.[0]?.payload?.content?.slice(0, 100) || ragResult.context.document?.payload?.content?.slice(0, 100) || 'N/A',
          firstParentContent: ragResult.context.parents[0]?.payload?.content?.slice(0, 100) || 'N/A',
          firstChildContent: ragResult.context.children[0]?.payload?.content?.slice(0, 100) || 'N/A',
        })

        const docCount = ragResult.context.documents && ragResult.context.documents.length > 0 ? ragResult.context.documents.length : ragResult.context.document ? 1 : 0
        sender.send({
          type: 'search',
          data: {
            count: ragResult.totalResults,
            breakdown: {
              document: docCount,
              parents: ragResult.context.parents.length,
              children: ragResult.context.children.length,
            },
            documentIds: sourceIds,
          },
        })

        citations.push(...buildAnswerCitations(ragResult.context))

        // ========== DeepSeek ç”Ÿæˆå›ç­” ==========
        const client = createLLMClient(modelKey)
        const finalMessages = buildDeepSeekMessages(currentQuery, ragResult.context, systemPrompt)

        let streamError: unknown = null
        try {
          await client.chatStream(finalMessages, {
            onEvent: (event) => {
              if (event.type === 'delta' && event.content && event.content !== '[FIRST_TOKEN]') {
                fullContent += event.content
                sender.token(event.content)
              }
            },
          })
        } catch (error) {
          streamError = error
        }

        if (streamError) {
          if (!fullContent) {
            const { content } = await client.chat(finalMessages)
            fullContent = content
          } else {
            console.warn(
              '[Chat] stream failed after partial output:',
              streamError instanceof Error ? streamError.message : String(streamError)
            )
          }
        }

        // ä¿å­˜ AI å›å¤
        const now = new Date().toISOString()
        const assistantMessageId = db
          .prepare(
            `
            INSERT INTO chat_messages (session_id, role, content, citations, created_at)
            VALUES (?, ?, ?, ?, ?)
          `
          )
          .run(
            sessionId,
            'assistant',
            fullContent,
            citations.length > 0 ? JSON.stringify(citations) : null,
            now
          ).lastInsertRowid

        sender.done({
          content: fullContent,
          citations,
          id: assistantMessageId,
        })
        } catch (error) {
          sender.error(error instanceof Error ? error.message : 'Unknown error')
        }
      }),
      { headers: getSSEHeaders() }
    )
  } catch (error) {
    return new Response(
      JSON.stringify({
        error: error instanceof Error ? error.message : 'Internal server error',
      }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }
}
